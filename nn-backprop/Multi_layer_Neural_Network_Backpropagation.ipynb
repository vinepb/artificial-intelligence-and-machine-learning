{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-layer_Neural_Network_Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wOlPACq8QzCA",
        "aKrjEw9xQ3LQ",
        "mf_3RCV0ZyoA",
        "HYgj98XkDabf",
        "UaRHvCTIXv3x",
        "bsIhyhnR0pw9",
        "aGkDU-G59PPB"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVMpLxf4Z-3L"
      },
      "source": [
        "# Multi-layer Neural Newtork with Backpropagation\n",
        "### Vinicius Pimenta Bernardo (202002447)\n",
        "### Thiago Werneck Ferreira dos Santos (202003651)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc-JvCZGZ8oL"
      },
      "source": [
        "# Import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm1ogXIxZ793"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List\n",
        "\n",
        "from decimal import *\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZDmdhJTYq7b"
      },
      "source": [
        "getcontext().prec = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0qEKaeKuGwH"
      },
      "source": [
        "$L$ is the number of layers, including the input and output layers, and $s_L^{(l)}$ is the number of neurons per layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOlPACq8QzCA"
      },
      "source": [
        "# Upload training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "deo9G_CJZ-BL",
        "outputId": "40af1bf9-ccdf-4a29-feda-167a2148a2a7"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# Upload classification2.txt, needs to be repeated with every new session"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d0e569f8-3e53-4936-850e-51149fa359b3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d0e569f8-3e53-4936-850e-51149fa359b3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving classification2.txt to classification2.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlB8v4c8aMbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0412f3ba-b325-4900-d41e-02914a9512b8"
      },
      "source": [
        "file_name = \"classification2.txt\"\n",
        "training_data = uploaded[file_name].decode(\"utf-8\").split(\"\\n\")\n",
        "# Parse data\n",
        "for i in range(len(training_data)):\n",
        "    training_data[i] = training_data[i].split(\",\")\n",
        "training_data = training_data[:-1]\n",
        "print(training_data)\n",
        "print(len(training_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['0.051267', '0.69956', '1'], ['-0.092742', '0.68494', '1'], ['-0.21371', '0.69225', '1'], ['-0.375', '0.50219', '1'], ['-0.51325', '0.46564', '1'], ['-0.52477', '0.2098', '1'], ['-0.39804', '0.034357', '1'], ['-0.30588', '-0.19225', '1'], ['0.016705', '-0.40424', '1'], ['0.13191', '-0.51389', '1'], ['0.38537', '-0.56506', '1'], ['0.52938', '-0.5212', '1'], ['0.63882', '-0.24342', '1'], ['0.73675', '-0.18494', '1'], ['0.54666', '0.48757', '1'], ['0.322', '0.5826', '1'], ['0.16647', '0.53874', '1'], ['-0.046659', '0.81652', '1'], ['-0.17339', '0.69956', '1'], ['-0.47869', '0.63377', '1'], ['-0.60541', '0.59722', '1'], ['-0.62846', '0.33406', '1'], ['-0.59389', '0.005117', '1'], ['-0.42108', '-0.27266', '1'], ['-0.11578', '-0.39693', '1'], ['0.20104', '-0.60161', '1'], ['0.46601', '-0.53582', '1'], ['0.67339', '-0.53582', '1'], ['-0.13882', '0.54605', '1'], ['-0.29435', '0.77997', '1'], ['-0.26555', '0.96272', '1'], ['-0.16187', '0.8019', '1'], ['-0.17339', '0.64839', '1'], ['-0.28283', '0.47295', '1'], ['-0.36348', '0.31213', '1'], ['-0.30012', '0.027047', '1'], ['-0.23675', '-0.21418', '1'], ['-0.06394', '-0.18494', '1'], ['0.062788', '-0.16301', '1'], ['0.22984', '-0.41155', '1'], ['0.2932', '-0.2288', '1'], ['0.48329', '-0.18494', '1'], ['0.64459', '-0.14108', '1'], ['0.46025', '0.012427', '1'], ['0.6273', '0.15863', '1'], ['0.57546', '0.26827', '1'], ['0.72523', '0.44371', '1'], ['0.22408', '0.52412', '1'], ['0.44297', '0.67032', '1'], ['0.322', '0.69225', '1'], ['0.13767', '0.57529', '1'], ['-0.0063364', '0.39985', '1'], ['-0.092742', '0.55336', '1'], ['-0.20795', '0.35599', '1'], ['-0.20795', '0.17325', '1'], ['-0.43836', '0.21711', '1'], ['-0.21947', '-0.016813', '1'], ['-0.13882', '-0.27266', '1'], ['0.18376', '0.93348', '0'], ['0.22408', '0.77997', '0'], ['0.29896', '0.61915', '0'], ['0.50634', '0.75804', '0'], ['0.61578', '0.7288', '0'], ['0.60426', '0.59722', '0'], ['0.76555', '0.50219', '0'], ['0.92684', '0.3633', '0'], ['0.82316', '0.27558', '0'], ['0.96141', '0.085526', '0'], ['0.93836', '0.012427', '0'], ['0.86348', '-0.082602', '0'], ['0.89804', '-0.20687', '0'], ['0.85196', '-0.36769', '0'], ['0.82892', '-0.5212', '0'], ['0.79435', '-0.55775', '0'], ['0.59274', '-0.7405', '0'], ['0.51786', '-0.5943', '0'], ['0.46601', '-0.41886', '0'], ['0.35081', '-0.57968', '0'], ['0.28744', '-0.76974', '0'], ['0.085829', '-0.75512', '0'], ['0.14919', '-0.57968', '0'], ['-0.13306', '-0.4481', '0'], ['-0.40956', '-0.41155', '0'], ['-0.39228', '-0.25804', '0'], ['-0.74366', '-0.25804', '0'], ['-0.69758', '0.041667', '0'], ['-0.75518', '0.2902', '0'], ['-0.69758', '0.68494', '0'], ['-0.4038', '0.70687', '0'], ['-0.38076', '0.91886', '0'], ['-0.50749', '0.90424', '0'], ['-0.54781', '0.70687', '0'], ['0.10311', '0.77997', '0'], ['0.057028', '0.91886', '0'], ['-0.10426', '0.99196', '0'], ['-0.081221', '1.1089', '0'], ['0.28744', '1.087', '0'], ['0.39689', '0.82383', '0'], ['0.63882', '0.88962', '0'], ['0.82316', '0.66301', '0'], ['0.67339', '0.64108', '0'], ['1.0709', '0.10015', '0'], ['-0.046659', '-0.57968', '0'], ['-0.23675', '-0.63816', '0'], ['-0.15035', '-0.36769', '0'], ['-0.49021', '-0.3019', '0'], ['-0.46717', '-0.13377', '0'], ['-0.28859', '-0.060673', '0'], ['-0.61118', '-0.067982', '0'], ['-0.66302', '-0.21418', '0'], ['-0.59965', '-0.41886', '0'], ['-0.72638', '-0.082602', '0'], ['-0.83007', '0.31213', '0'], ['-0.72062', '0.53874', '0'], ['-0.59389', '0.49488', '0'], ['-0.48445', '0.99927', '0'], ['-0.0063364', '0.99927', '0'], ['0.63265', '-0.030612', '0']]\n",
            "118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5ObSWr0ZcLI"
      },
      "source": [
        "reference for this section:\n",
        "\n",
        "https://buomsoo-kim.github.io/colab/2018/04/15/Colab-Importing-CSV-and-JSON-files-in-Google-Colab.md/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKrjEw9xQ3LQ"
      },
      "source": [
        "# Weights initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhaqysNuNtQj"
      },
      "source": [
        "There is a separate weight matrix for each layer $l \\in L,\\space l>1$ so, there will be a matrx $w^{(L)}, \\space w^{(L-1)}, \\space w^{(L-2)}, \\space ..., w^{(2)}$\n",
        "\n",
        "A matrix $w^{(l)}$ will be used to calculate the activation for that same layer $l$ using the activations from layer $l-1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hw5ZJPNyLL-"
      },
      "source": [
        "def initializeWeights(layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns a list of arrays containing random weights\n",
        "    \"\"\"\n",
        "    weights = []\n",
        "    for l in range(1, len(layers)):\n",
        "        weights.append(np.random.rand(layers[l], layers[l-1]+1))\n",
        "    return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf_3RCV0ZyoA"
      },
      "source": [
        " # Foward propagation function definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnXB3xoWi3Et"
      },
      "source": [
        "$z^{(l)}_i=\\sum_{j=0}^{s_L^{(l-1)}}w_{ij}^{(l)}y_{j}^{(l-1)}$\n",
        "\n",
        "for a given $i$, $w_{ij}^{(l)}$ is the line ```[i:]``` from the weight matrix of layer $(l)$, $y_j^{(l-1)}$ is the column o activation values of layer $(l-1)$, and $s_L^{(n)}$ is the number of neurons on a given layer $n$\n",
        "\n",
        "$\\begin{pmatrix}\n",
        "  w^{(l)}_{i,0} & w^{(l)}_{i,1} & \\cdots & w^{(l)}_{i,s_L^{(l-1)}}\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "  1\\\\\n",
        "  y_{1}^{(l-1)}\\\\\n",
        "  \\vdots\\\\\n",
        "  y_{s_L^{(l-1)}}^{(l-1)}\n",
        "\\end{pmatrix}=z_i^{(l)}$\n",
        "\n",
        "and the activation $y_i^{(l)}$ is $g(z_i^{(l)})=\\frac{1}{1+e^{z_i^{(l)}}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGfqO8d0PJ6b"
      },
      "source": [
        "def transfer(weights: np.ndarray, activations: np.ndarray) ->float:\n",
        "    \"\"\"\n",
        "    Single neuron weighted sum\n",
        "    \"\"\"\n",
        "    transfer = weights[0] # bias weight * 1\n",
        "    transfer += np.matmul(weights[1:], activations)[0]\n",
        "    return transfer\n",
        "\n",
        "def activation(transfer: float) ->float:\n",
        "    \"\"\"\n",
        "    Single neuron activation function\n",
        "    \"\"\"\n",
        "    return (1/(1+math.exp(-transfer)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhMflICxsKaG"
      },
      "source": [
        "compute $z^{(1)}$ and $y^{(1)}$ using $x^{(m)}$ for one $m$ corresponding to a single training set and then $z^{(l)}$ and $y^{(l)}$ using $y^{(l-1)}$ for $l\\in2...L$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H6sputTe6VJ"
      },
      "source": [
        "def fowardPropagate(inputs: np.ndarray, weights: List[np.ndarray], layers: List) ->(List[np.ndarray], List[np.ndarray]):\n",
        "    \"\"\"Propagate the input throughout the network\n",
        "\n",
        "    The second layer activation is calculated using the inputs, which are the activation of the first layer,\n",
        "    and the remaining layers activation values are calculated using the values from previous layer\n",
        "    @param inputs: A single set of inputs from the training set as a column\n",
        "    @param weights: The network neurons weights\n",
        "    @param layers: List containing number of units per layer\n",
        "    @return neuronsNetSum, neuronsActivation: The propagated transfer and activation values of neurons\n",
        "    \"\"\"\n",
        "    neuronsTransfer = []\n",
        "    neuronsActivation = []\n",
        "    layerTransfer = np.array([[transfer(w[0][i,:], inputs)] for i in range(layers[1])])\n",
        "    layerActivation = np.array([[activation(layerTransfer[i])] for i in range(layers[1])])\n",
        "    neuronsTransfer.append(layerTransfer)\n",
        "    neuronsActivation.append(layerActivation)\n",
        "    \n",
        "    for l in range(2, len(layers)):\n",
        "        layerTransfer = np.array([[transfer(w[l-1][i,:], neuronsActivation[l-2])] for i in range(layers[l])])\n",
        "        layerActivation = np.array([[activation(layerTransfer[i])] for i in range(layers[l])])\n",
        "        neuronsTransfer.append(layerTransfer)\n",
        "        neuronsActivation.append(layerActivation)\n",
        "\n",
        "    return neuronsTransfer, neuronsActivation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYgj98XkDabf"
      },
      "source": [
        "# Error calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdUQNWVUTDKd"
      },
      "source": [
        "$J(w) = \\frac{1}{2}*\\sum_{i=1}^{s_L^{(L)}}(y_i^{(L)}-t_i)^2$\n",
        "\n",
        "Considering the error function as:\n",
        "\n",
        "$E = \\frac{1}{2}*(y_i^{(L)}-t_i)^2$\n",
        "\n",
        "for a single neuron $i$ on the output layer, and as:\n",
        "\n",
        "$E=\\frac{1}{2n}\\sum_{m=1}^n (y^{(L)}_i-t^{(m)}_i)$\n",
        "\n",
        "for $n$ training examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urah2l0WDiVG"
      },
      "source": [
        "def calculateSumError(outputs: np.ndarray, expected: np.ndarray, layers: List) ->float:\n",
        "    \"\"\"\n",
        "    Returns the accumulated error J(w) of the last layer\n",
        "    \"\"\"\n",
        "    sumError = 0.0\n",
        "    for i in range(layers[-1]):\n",
        "        sumError += ((outputs[i][0] - expected[i][0]) ** 2) * 0.5\n",
        "    return sumError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaRHvCTIXv3x"
      },
      "source": [
        "# Backpropagatin function definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwmM-ZQafn-Y"
      },
      "source": [
        "Considering the error function as:\n",
        "\n",
        "$E = \\frac{1}{2}*(y_i^{(L)}-t_i)^2$\n",
        "\n",
        "for a single neuron $i$ on the output layer.\n",
        "\n",
        "The derivative of the error is:\n",
        "\n",
        "$\\frac{\\delta E}{\\delta w^{(l)}_{ij}} = \\frac{\\delta E}{\\delta y^{(l)}_i}\\frac{\\delta y^{(l)}_i}{\\delta z^{(l)}_i}\\frac{\\delta z^{(l)}_i}{\\delta w^{(l)}_{ij}}=\\frac{\\delta E}{\\delta y^{(l)}_i}\\frac{\\delta y^{(l)}_i}{\\delta z^{(l)}_i}y^{(l-1)}_j=\\delta^{(l)}_iy^{(l-1)}_j$\n",
        "\n",
        "$\\frac{\\delta y_i^{(l)}}{\\delta z_i^{(l)}} = y_i^{(l)}(1-y_i^{(l)})$ when $y_i^{(l)}$ is $g(z_i^{(l)})=\\frac{1}{1+e^{-z_i^{(l)}}}$\n",
        "\n",
        "$\\delta^{(l)}_j=\\frac{\\delta E}{\\delta y^{(l)}_j}\\frac{\\delta y^{(l)}_j}{\\delta z^{(l)}_j}=\\begin{cases}(y^{(l)}_j-t_j)y^{(l)}_j(1-y^{(l)}_j), \\space l=L \\\\ (\\sum_{i=1}^{s_L^{(l+1)}}w_{ij}^{(l+1)}\\delta_{i}^{(l+1)})y^{(l)}_j(1-y^{(l)}_j), \\space l<L\\end{cases}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl4F7CpyXtTt"
      },
      "source": [
        "def activationDerivate(activation: float) ->float:\n",
        "    \"\"\"\n",
        "    Single neuron activation derivate function\n",
        "    \"\"\"\n",
        "    return activation*(1.0 - activation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2FS3W9mRgVV"
      },
      "source": [
        "def backPropagateError(outputs: np.ndarray, activations: List[np.ndarray], weights: List[np.ndarray], layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"Backpropagate the error throughout the network\n",
        "\n",
        "    The last layer error is calculated using the expected outputs from the training examples,\n",
        "    and the remaining layers error values are calculated using the values from upfront layer\n",
        "    @param outputs: A single set of outputs from the training set as a column\n",
        "    @param activation: Calculated using the foward propagation\n",
        "    @param weights: The network neurons weights\n",
        "    @param layers: List containing number of units per layer\n",
        "    @return neuronsErrorDerivatives: The backpropagated error values of neurons\n",
        "    \"\"\"\n",
        "    neuronsErrorDerivatives = []\n",
        "    layerErrorDerivative = np.array([[((activations[len(layers)-2][i,0] - outputs[i,0])*activationDerivate(activations[len(layers)-2][i,0]))] for i in range(layers[len(layers)-1])])\n",
        "    neuronsErrorDerivatives.append(layerErrorDerivative)\n",
        "\n",
        "    for l in reversed(range(1, len(layers)-1)):\n",
        "        layerErrorDerivative = np.matmul(np.transpose(weights[l][:,1:]), neuronsErrorDerivatives[0])*activationDerivate(activations[l-1])\n",
        "        neuronsErrorDerivatives.insert(0, layerErrorDerivative)\n",
        "\n",
        "    return neuronsErrorDerivatives"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsIhyhnR0pw9"
      },
      "source": [
        "# Update weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XskazuECS_S7"
      },
      "source": [
        "$\\Delta^{(l)}_{ij} = \\Delta^{(l)}_{ij}+y^{(l-1)}_j \\delta^{(l)}_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlN5vnDp4Jwx"
      },
      "source": [
        "def initializeDelta(layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns a list of arrays containing zeros\n",
        "    \"\"\"\n",
        "    Delta = []\n",
        "    for l in range(1, len(layers)):\n",
        "        Delta.append(np.zeros((layers[l], layers[l-1]+1)))\n",
        "    return Delta\n",
        "\n",
        "def updateDelta(inputs: np.ndarray, Delta: List[np.ndarray], activations: List[np.ndarray], delta: List[np.ndarray], layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns the updated value of Delta using delta and activation values\n",
        "    \"\"\"\n",
        "    updatedDelta = []\n",
        "    updatedDelta.append(Delta[0] + np.matmul(delta[0], np.array([np.append(np.array([[1]]), inputs)]) ) )\n",
        "    for l in range(2, len(layers)):\n",
        "        updatedDelta.append(Delta[l-1] + np.matmul(delta[l-1], np.array([np.append(np.array([[1]]), activations[l-2])]) ) )\n",
        "    return updatedDelta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LriHK922lORh"
      },
      "source": [
        "$D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij}+\\lambda w^{(l)}_{ij}$ for $j\\neq 0$\n",
        "\n",
        "and\n",
        "\n",
        "$D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij}$ for $j=0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFg40anQdWEx"
      },
      "source": [
        "def regRateMatrix(regRate: float, layers: List) ->List[np.array]:\n",
        "    \"\"\"\n",
        "    Returns the regularization matrix with the first columns being 0.0 and lambda for the rest\n",
        "    \"\"\"\n",
        "    regRates = []\n",
        "    for l in range(1, len(layers)):\n",
        "        line = np.append(np.array([0.0]), np.linspace(regRate, regRate, layers[l-1]))\n",
        "        grid = np.array([line])\n",
        "        for _ in range(1, layers[l]):\n",
        "            grid = np.append(grid, [line], axis=0)\n",
        "        regRates.append(grid)\n",
        "\n",
        "    return regRates\n",
        "\n",
        "def calculateWeightAdjust(Delta: List[np.ndarray], weights: List[np.ndarray], layers: List, num_examples: float, regRate: float=0.0) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns matrixes containing the cost derivate for each weight\n",
        "    \"\"\"\n",
        "    weightsAdjust = []\n",
        "    regRates = regRateMatrix(regRate, layers)\n",
        "    for l in range(1, len(layers)):\n",
        "        layerWeightAdjust = Delta[l-1]*(1.0/num_examples) + weights[l-1]*regRates[l-1]\n",
        "        weightsAdjust.append(layerWeightAdjust)\n",
        "    \n",
        "    return weightsAdjust"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGkDU-G59PPB"
      },
      "source": [
        "# Approximate gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMTNFym_9Tag"
      },
      "source": [
        "def initializeGradApprox(layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns a list of arrays containing zeros\n",
        "    \"\"\"\n",
        "    gradApprox = []\n",
        "    for l in range(1, len(layers)):\n",
        "        gradApprox.append(np.zeros((layers[l], layers[l-1]+1), dtype=Decimal))\n",
        "    return gradApprox\n",
        "\n",
        "def transferNumerical(weights: np.ndarray, activations: np.ndarray) ->Decimal:\n",
        "    \"\"\"\n",
        "    Single neuron weighted sum\n",
        "    \"\"\"\n",
        "    transfer = Decimal(weights[0]) # bias weight * 1\n",
        "    transfer += Decimal(np.matmul(weights[1:], activations)[0])\n",
        "    return transfer\n",
        "\n",
        "def activationNumerical(transfer: Decimal) ->Decimal:\n",
        "    \"\"\"\n",
        "    Single neuron activation function\n",
        "    \"\"\"\n",
        "    return (Decimal(1.0)/Decimal((1.0+math.exp(-transfer))))\n",
        "\n",
        "def calculateSumErrorNumerical(outputs: np.ndarray, expected: np.ndarray, layers: List) ->Decimal:\n",
        "    sumError = Decimal(0.0)\n",
        "    for i in range(layers[-1]):\n",
        "        sumError += Decimal(((outputs[i][0] - expected[i][0]) ** 2)) * Decimal(0.5)\n",
        "    return sumError\n",
        "\n",
        "def activationDerivateNumerical(activation: float) ->Decimal:\n",
        "    \"\"\"\n",
        "    Single neuron activation derivate function\n",
        "    \"\"\"\n",
        "    return Decimal(activation)*(Decimal(1.0 - activation))\n",
        "\n",
        "def fowardPropagateNumerical(inputs: np.ndarray, weights: List[np.ndarray], layers: List) ->(List[np.ndarray], List[np.ndarray]):\n",
        "    \"\"\"Propagate the input throughout the network with Decimal instead of float\n",
        "\n",
        "    The second layer activation is calculated using the inputs, which are the activation of the first layer,\n",
        "    and the remaining layers activation values are calculated using the values from previous layer\n",
        "    @param inputs: A single set of inputs from the training set as a column\n",
        "    @param weights: The network neurons weights\n",
        "    @param layers: List containing number of units per layer\n",
        "    @return neuronsNetSum, neuronsActivation: The propagated transfer and activation values of neurons\n",
        "    \"\"\"\n",
        "    neuronsTransfer = []\n",
        "    neuronsActivation = []\n",
        "    layerTransfer = np.array([[transfer(w[0][i,:], inputs)] for i in range(layers[1])], dtype=Decimal)\n",
        "    layerActivation = np.array([[activation(layerTransfer[i])] for i in range(layers[1])], dtype=Decimal)\n",
        "    neuronsTransfer.append(layerTransfer)\n",
        "    neuronsActivation.append(layerActivation)\n",
        "    \n",
        "    for l in range(2, len(layers)):\n",
        "        layerTransfer = np.array([[transfer(w[l-1][i,:], neuronsActivation[l-2])] for i in range(layers[l])], dtype=Decimal)\n",
        "        layerActivation = np.array([[activation(layerTransfer[i])] for i in range(layers[l])], dtype=Decimal)\n",
        "        neuronsTransfer.append(layerTransfer)\n",
        "        neuronsActivation.append(layerActivation)\n",
        "\n",
        "    return neuronsTransfer, neuronsActivation\n",
        "\n",
        "def calculateGradApprox(weights: List[np.ndarray], inputs: np.ndarray, expected: np.ndarray, layers: List) ->List[np.ndarray]:\n",
        "    epsilon = 0.00001\n",
        "    gradApprox = []\n",
        "    for l in range(1, len(layers)):\n",
        "        layer_weights = np.zeros((layers[l], layers[l-1]+1), dtype=Decimal)\n",
        "        for i in range(0, layers[l]):\n",
        "            for j in range(0, layers[l-1]+1):\n",
        "                w_plus = np.copy(weights)\n",
        "                w_minus = np.copy(weights)\n",
        "                w_plus[l-1][i,j] += epsilon\n",
        "                w_minus[l-1][i,j] -= epsilon\n",
        "                z_plus, y_plus = fowardPropagateNumerical(inputs, w_plus, layers)\n",
        "                sumError_plus = calculateSumErrorNumerical(y_plus[-1], expected, layers)\n",
        "                z_minus, y_minus = fowardPropagateNumerical(inputs, w_minus, layers)\n",
        "                sumError_minus = calculateSumErrorNumerical(y_minus[-1], expected, layers)\n",
        "                dJ_dwij = (Decimal(sumError_plus) - Decimal(sumError_minus)) / Decimal((2.0 * epsilon))\n",
        "                layer_weights[i, j] += dJ_dwij\n",
        "        gradApprox.append(layer_weights)\n",
        "    return gradApprox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwuzDWktDDmv"
      },
      "source": [
        "# Run training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1KvQ4iVqaWA"
      },
      "source": [
        "### Compare numerical and standard gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-vGbGdT5zAt",
        "outputId": "d66760b0-e57a-4268-abd6-de3fab9f1e0f"
      },
      "source": [
        "######################################################################\n",
        "# Run one epoch to compare numerical gradient with calculated derivate\n",
        "######################################################################\n",
        "# s_l always needs 2 at beginning and 1 at the end to comply with data format\n",
        "# s_l = [2, 1]\n",
        "s_l = [2, 3, 3, 1]\n",
        "initial_weights = initializeWeights(s_l)\n",
        "w = initial_weights.copy()\n",
        "lambdaRate = 0.0\n",
        "alphaRate = 1.0\n",
        "costsList = []\n",
        "\n",
        "# num_training = len(training_data)\n",
        "num_training = 100\n",
        "\n",
        "print(\"Network shape: s_l = \", end='')\n",
        "print(s_l)\n",
        "print(\"Training sets used: \" + str(num_training))\n",
        "print(\"Alpha rate: \" + str(alphaRate) + \" Lambda rate: \" + str(lambdaRate))\n",
        "print(\"\")\n",
        "\n",
        "print(\"Initial weights\")\n",
        "for s in range(len(w)):\n",
        "    print(\"w^(%s) \" % (s+2))\n",
        "    print(w[s])\n",
        "\n",
        "# Init stuff with 0\n",
        "Delta = initializeDelta(s_l)\n",
        "gradApproxSum = initializeGradApprox(s_l)\n",
        "cost = 0\n",
        "# Run one epoch\n",
        "for m in range(num_training):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    cost += calculateSumError(y[-1], t, s_l)\n",
        "    delta = backPropagateError(t, y, w, s_l) \n",
        "    Delta = updateDelta(x, Delta, y, delta, s_l)\n",
        "    gradApprox = calculateGradApprox(w, x, t, s_l)\n",
        "    for s in range(len(gradApprox)):\n",
        "        gradApproxSum[s] = gradApproxSum[s] + gradApprox[s]\n",
        "cost = cost/num_training\n",
        "costsList.append(cost)\n",
        "D = calculateWeightAdjust(Delta, w, s_l, num_training, lambdaRate)\n",
        "\n",
        "# Print both derivates (calculated and numerical)\n",
        "print(\"\\nCost derivates\")\n",
        "for s in range(len(D)):\n",
        "    print(\"D^(%s) \" % (s+2))\n",
        "    print(D[s])\n",
        "for s in range(len(gradApproxSum)):\n",
        "    gradApproxSum[s] = gradApproxSum[s] * Decimal((1.0/num_training))\n",
        "print(\"\\nApproximate derivates (numerical method)\")\n",
        "for s in range(len(gradApproxSum)):\n",
        "    print(\"D_approx^(%s) \" % (s+2))\n",
        "    print(gradApproxSum[s])\n",
        "\n",
        "# Adjust weights using derivate\n",
        "for s in range(len(w)):\n",
        "    w[s] = np.copy(w[s] - alphaRate * D[s])\n",
        "print(\"\\nFinal weights after training\")\n",
        "for s in range(len(w)):\n",
        "    print(\"w^(%s) \" % (s+2))\n",
        "    print(w[s])\n",
        "\n",
        "# Caculate the cost one more time for final weights\n",
        "cost = 0\n",
        "for m in range(num_training):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    cost += calculateSumError(y[-1], t, s_l)\n",
        "cost = cost/num_training\n",
        "costsList.append(cost)\n",
        "\n",
        "print(\"\\nCosts for all epochs\")\n",
        "print(costsList)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network shape: s_l = [2, 3, 3, 1]\n",
            "Training sets used: 100\n",
            "Alpha rate: 1.0 Lambda rate: 0.0\n",
            "\n",
            "Initial weights\n",
            "w^(2) \n",
            "[[0.76890752 0.61723569 0.29711479]\n",
            " [0.45473856 0.09396039 0.91075803]\n",
            " [0.14828925 0.00549917 0.4469605 ]]\n",
            "w^(3) \n",
            "[[0.09416079 0.1708389  0.42771098 0.69539482]\n",
            " [0.4089891  0.51707435 0.84838957 0.89955105]\n",
            " [0.81014526 0.89925213 0.10174644 0.57163114]]\n",
            "w^(4) \n",
            "[[0.49278337 0.78461041 0.06438325 0.38850874]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:792: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, order=order, subok=subok, copy=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cost derivates\n",
            "D^(2) \n",
            "[[4.85002135e-04 1.24137201e-04 7.86920625e-05]\n",
            " [5.89547668e-04 2.12324010e-04 5.24324299e-05]\n",
            " [1.23256172e-03 4.27562964e-04 2.31061909e-04]]\n",
            "D^(3) \n",
            "[[0.00569741 0.00418193 0.00371457 0.0031902 ]\n",
            " [0.00026738 0.00019498 0.00017075 0.0001478 ]\n",
            " [0.0015997  0.00116895 0.00103929 0.00089414]]\n",
            "D^(4) \n",
            "[[0.03529393 0.02502341 0.03045698 0.03052703]]\n",
            "\n",
            "Approximate derivates (numerical method)\n",
            "D_approx^(2) \n",
            "[[Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]\n",
            " [Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]\n",
            " [Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]]\n",
            "D_approx^(3) \n",
            "[[Decimal('0E-59') Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]\n",
            " [Decimal('0E-59') Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]\n",
            " [Decimal('0E-59') Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]]\n",
            "D_approx^(4) \n",
            "[[Decimal('0E-59') Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]]\n",
            "\n",
            "Final weights after training\n",
            "w^(2) \n",
            "[[0.76842252 0.61711155 0.2970361 ]\n",
            " [0.45414901 0.09374807 0.9107056 ]\n",
            " [0.14705669 0.0050716  0.44672944]]\n",
            "w^(3) \n",
            "[[0.08846337 0.16665697 0.4239964  0.69220463]\n",
            " [0.40872171 0.51687937 0.84821883 0.89940325]\n",
            " [0.80854556 0.89808318 0.10070715 0.57073699]]\n",
            "w^(4) \n",
            "[[0.45748943 0.759587   0.03392627 0.3579817 ]]\n",
            "\n",
            "Costs for all epochs\n",
            "[0.14792161388952857, 0.14413234153788748]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXj37Cfr31er"
      },
      "source": [
        "### Train network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MybmtkiwLASq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "9655074a-9031-4f16-f785-881e0cf1e331"
      },
      "source": [
        "# Initialize weights and plot initial decision frontier\n",
        "# s_l always needs 2 at beginning and 1 at the end to comply with data format\n",
        "# s_l = [2, 1]\n",
        "s_l = [2, 3, 3, 1]\n",
        "initial_weights = initializeWeights(s_l)\n",
        "w = initial_weights.copy()\n",
        "\n",
        "# Plotando fronteira de decisão\n",
        "x1s = np.linspace(-1,1.5,50)\n",
        "x2s = np.linspace(-1,1.5,50)\n",
        "zs=np.zeros((len(x1s),len(x2s)))\n",
        "\n",
        "for i in range(len(x1s)):\n",
        "    for j in range(len(x2s)):\n",
        "        xs = np.array([[x1s[i]],[x2s[j]]])\n",
        "        zss, yss = fowardPropagate(xs, w, s_l)\n",
        "        zs[i,j] = zss[-1][0,0] # saida do modelo antes de aplicar a função sigmoide \n",
        "plt.contour(x1s,x2s,np.transpose(zs),0)\n",
        "\n",
        "df=pd.read_csv(\"classification2.txt\", header=None)\n",
        "\n",
        "ins=df.iloc[:,:-1].values\n",
        "outs=df.iloc[:,-1].values\n",
        "pos , neg = (outs==1).reshape(118,1) , (outs==0).reshape(118,1)\n",
        "plt.scatter(ins[pos[:,0],0],ins[pos[:,0],1],c=\"r\",marker=\"+\")\n",
        "plt.scatter(ins[neg[:,0],0],ins[neg[:,0],1],marker=\"o\",s=10)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.legend([\"Accepted\",\"Rejected\"],loc=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7feee45efe10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8deHEDaFABERRRZbUBBZA15kVRtZZJFNQFu1Vy9Qq/b23nqvtrdq7aLV+mvrUpGKtS6EfZcqCgZRVAgVBbGKqJRQBAQNiSyyfH9/ZBICJGGSmbPMzPv5eOSRWc6c+czJ5HzOdzmfY845REREqqtG0AGIiEhiUyIREZGYKJGIiEhMlEhERCQmSiQiIhITJRIREYlJoInEzJ4ys51mtqGC5/ubWYGZrYv83OV3jCIiUrmaAb//08CjwDOVLLPSOTfEn3BERKSqAm2ROOdeA/YEGYOIiMQm6BZJNHqa2bvAv4CfOOfeP3EBM5sATAA47bTTul1wwQU+hygiktjWrl37hXOuSXVeG/ZE8negpXOuyMwGA/OBNicu5JybAkwByMrKcnl5ef5GKSKS4MxsS3VfG+pZW865vc65osjtJUC6mZ0RcFgiIlJGqBOJmZ1lZha53YPieHcHG5WIiJQVaNeWmeUA/YEzzCwfuBtIB3DOTQZGAz8ws8PAfmCcU7liEZFQCTSROOfGn+L5RymeHiwiwqFDh8jPz+fAgQNBh5Kw6tSpQ/PmzUlPT4/bOsM+2C4iUio/P5/69evTqlUrIr3eUgXOOXbv3k1+fj6tW7eO23pDPUYiIlLWgQMHyMzMVBKpJjMjMzMz7i06JRIRSShKIrHxYvspkYiISEyUSEREqmj+/PmYGf/4xz98eb/f/OY3VX7N008/zS233OJBNCdTIhERqaKcnBx69+5NTk6OL+9XnUTiJyUSEUlu/fsX/8RJUVERr7/+OlOnTmX69OkAHDlyhJ/85Cd06NCBjh078sgjjwCwZs0aLrnkEjp16kSPHj0oLCzkyJEj3H777XTv3p2OHTvyxBNPAJCbm0vfvn258sorOf/885k0aRJHjx7ljjvuYP/+/XTu3Jlrr70WgOeee44ePXrQuXNnJk6cyJEjRwD4y1/+Qtu2benRowdvvPFG3D7zqWj6r4hIFSxYsICBAwfStm1bMjMzWbt2LatXr+azzz5j3bp11KxZkz179vDNN98wduxYZsyYQffu3dm7dy9169Zl6tSpZGRksGbNGg4ePEivXr244oorAFi9ejUbN26kZcuWDBw4kLlz53L//ffz6KOPsm7dOgA++OADZsyYwRtvvEF6ejo333wzzz//PNnZ2dx9992sXbuWjIwMLr30Urp06eLLNlEiEZHkVNIKWbHi+Pu5uTGtNicnhx/96EcAjBs3jpycHD799FMmTZpEzZrFu9TGjRuzfv16mjVrRvfu3QFo0KABAEuXLuW9995j9uzZABQUFLBp0yZq1apFjx49OO+88wAYP348r7/+OqNHjz7u/ZctW8batWtL17t//37OPPNM3n77bfr370+TJsUFfMeOHctHH30U02eNlhKJiEiU9uzZw/Lly1m/fj1mxpEjRzCz0p16NJxzPPLIIwwYMOC4x3Nzc0+amlveVF3nHNdffz333XffcY/Pnz+/Cp8kvjRGIiLJKTe3+Kdfv+KfkvsxmD17Nt/73vfYsmULn332GVu3bqV169Z06tSJJ554gsOHDwPFCef8889n+/btrFmzBoDCwkIOHz7MgAEDePzxxzl06BAAH330EV9//TVQ3LX16aefcvToUWbMmEHv3r0BSE9PL13+8ssvZ/bs2ezcubP0vbZs2cLFF1/MihUr2L17N4cOHWLWrFkxfdaqUCIREYlSTk4OI0aMOO6xUaNGsX37dlq0aEHHjh3p1KkT06ZNo1atWsyYMYNbb72VTp06kZ2dzYEDB7jpppto3749Xbt2pUOHDkycOLE0AXXv3p1bbrmFdu3a0bp169L3mjBhAh07duTaa6+lffv2/OpXv+KKK66gY8eOZGdns337dpo1a8Y999xDz5496dWrF+3atfNtu1iyFdPVha1EktcHH3zg6w7ST7m5ufzud79j8eLFnr9XedvRzNY657Kqsz61SEREJCYabBcRCYH+/fvTP47nu/hJLRIREYmJEomIiMREiURERGKiRCIiIjFRIhERqYK0tDQ6d+5Mhw4dGDp0KF999VWly0+ePJlnnnmmyu/z1Vdf8ac//anKr7vnnnv43e9+V+XXxUKJRESkCurWrcu6devYsGEDjRs35rHHHqt0+UmTJnHddddV+X2qm0iCoEQiIlJNPXv2ZNu2bQBs3ryZgQMH0q1bN/r06VN60auyLYSKltmxYwcjRoygU6dOdOrUiVWrVnHHHXewefNmOnfuzO233w7Agw8+WFp+/u677y6N49e//jVt27ald+/efPjhh35uAkDnkYhIknt54w5WbtpFnzZNyG7fNG7rPXLkCMuWLePGG28EisuYTJ48mTZt2vD2229z8803s3z58uNeU9Eyt912G/369WPevHkcOXKEoqIi7r//fjZs2FBaPn7p0qVs2rSJ1atX45xj2LBhvPbaa5x22mlMnz6ddevWcfjwYbp27Uq3bt3i9jmjoUQiIknr5Y07uC3nHfYfOsKsvHweHt8l5mRScpGpbdu20a5dO7KzsykqKmLVqlWMGTOmdLmDBw8e97rKllm+fHnpOEpaWhoZGRl8+eWXx71+6dKlLF26tPQaI0VFRWzatInCwkJGjBhBvXr1ABg2bFhMn686lEhEJGmt3LSL/YeKrx64/9ARVm7aFXMiKRkj2bdvHwMGDOCxxx7jhhtuoGHDhqWth/IcPXr0lMtUxjnHnXfeycSJE497/A9/+EO11hdPGiMRkaTVp00T6qanAVA3PY0+bZrEbd316tXj4Ycf5qGHHqJevXq0bt26tHS7c4533333uOUbNGhQ4TKXX345jz/+OFDcZVZQUED9+vUpLCwsff2AAQN46qmnKCoqAmDbtm3s3LmTvn37Mn/+fPbv309hYSGLFi2K22eMlhKJJKSXN+7grgUbeHnjjqBDkRDLbt+Uh8d34bqeLePSrXWiLl260LFjR3Jycnj++eeZOnUqnTp14sILL2TBggWly5VcoKqiZf74xz/y6quvctFFF9GtWzc2btxIZmYmvXr1okOHDtx+++1cccUVXHPNNfTs2ZOLLrqI0aNHU1hYSNeuXRk7diydOnVi0KBBVbrIVryojLwknLL93nXT0zzZQUg4JWIZ+VtvvZWuXbvy/e9/P+hQSqmMvKS88vq9E5VaVsnt5z//OW+//XYgA+B+UiKRhONlv7efSlpWz7y5hdty3lEySUK//OUvWb16NZmZmUGH4inN2pKEU9Lv7cW5AX7yYkZRKnDOlY45SNV5MZyhRCIJKbt904Tf6fZp04RZefmlYz2J2rLyU506ddi9ezeZmZlKJtXgnGP37t3UqVMnrutVIhEJSLK0rPzUvHlz8vPz2bUrccfFglanTh2aN28e13UqkYgEKBlaVn5KT0+ndevWQYchJ9Bgu4iIxCTQRGJmT5nZTjPbUMHzZmYPm9nHZvaemXX1O0YREalc0C2Sp4GBlTw/CGgT+ZkAPO5DTElF5yloG4h4LdBE4px7DdhTySLDgWdcsbeAhmbWzJ/oEl+ynadQnYSQbNtAJIyCbpGcyjnA1jL38yOPHcfMJphZnpnlaTbHMYGeAd6/f/FPnFQ3ISTTWfAiYRX2RBIV59wU51yWcy6rSRPNxS+RLGeAQ/UTQjJtA5GwCvv0323AuWXuN488JlEI5DyFklbIihXH38/NjWm11T15T+dqiHgv7IlkIXCLmU0HLgYKnHPbA44poSTLeQqxJIRk2QYiYRVoIjGzHKA/cIaZ5QN3A+kAzrnJwBJgMPAxsA8ITx1mKV9JyyNOLZGywpwQvLouuEgiCDSROOfGn+J5B/zQp3CkjFTaMcb6Wb24LrhIIgl715ZUQyh2jHFsiXgpHp9VVXwl1SXFrC05Jh7nTaTSlNl4fNZUnRmmEz2lhBJJktGOsWri8Vm9vi54GOlETylLXVtJJh7XuEilKbPx+qxhngjgBXXnSVlKJElGO8aqS6XPGi+6KJeUZV5cdjFIWVlZLi8vL+gwRJJeKs3sSwVmttY5l1Wd16pFIiLVopaclFAikeThwUmQYaYWgYSFZm1JuMS5anCy0qwpCRO1SCTxeVQoMsw0a0rCRIlE4iPWnXcKJoNYaNaUhIkSiSQ+jwpFxqPUjFdjGKl0ro+En6b/SmxObEn061f8O9aWSXVeH8dEUrYGV930tCqfsR7r60X8Fsv0Xw22S/LIzY1bayTWUjNhrlemGlkSb+raktjEu1spJGMisY5BhHUMQyXvxQtKJCLliHUMIqxjGLHO9tK5K1IejZGIpJBYxm407pPcVCIlhcXzCDHQo82gpvum2DTjWFpKOndFKqJEksDi2d/ta995vM45Scadvw+frbo1ssI67iPBUyLxWTyP+uN5hBjY0WZQJyLqBMgqC+u4jwRPicRH8T7qj+cRoi9Hm+XtvNetg86dq/96SI6dfzw/m4fbRRV/pTxKJD6K91F/PI8QAzva7Ny5eIfXsGHxfb+Sgkdnw0t8aZZYYlAi8ZEXR/3xPEL0/Ggz1p13Mu/84/HZkqzFpnNeEocSiY8CO+oP8w6lJLaCguPv+90ykbgI6xigeEuJxGfqYyZpzn73RCyfLeAWW5jHAMVbSiTJLBG6OpK5uypKyTIOEOYxQPGWEknYpfAONhV4Mg4Q0Hcl7GOA4h0lkmSWSEf7YY7NQycdxT8whezPloVue0TTalILInUpkYRVInRLpaI4/x1OOoov+Cwu642nqrSa1IJITUokqUDJJ7RKj+IfmEKfgs/Inv9U8RMhOnDQ7Ck5FSWSsEqkbimvVPTZg9gmHrYQs9s3Le7OCinNnpJTUSIRiUW8L+gVwgMHjX3IqSiR+CCm6Z0h2qH4pqKj/xJBjBtVtKM/MbayQpgUqktjH1IZJRKPqcxDkvKqqysJko6knkATiZkNBP4IpAFPOufuP+H5G4AHgW2Rhx51zj3pa5AxSuiByqCOqE/VzRPkkX5VWiKacScpIrBEYmZpwGNANpAPrDGzhc65jScsOsM5d4vvAcaJBiqTVIjHNKotgM+SLGf1p7ogWyQ9gI+dc58AmNl0YDhwYiJJaPEaqPT1Hy4sR9QVvV/Yd9rJmGQ8oG7f5BFkIjkH2Frmfj5wcTnLjTKzvsBHwI+dc1tPXMDMJgATAFq0aOFBqLGJdaCyyv9wuv65f5LhswZ04JDQ3b5ynLAPti8CcpxzB81sIvBX4LITF3LOTQGmAGRlZTl/Q/Se7/9wOqKOD223SqnbN3kEmUi2AeeWud+cY4PqADjndpe5+yTwgA9xhU7U/3C6/rlUhx8HDuWsW+enJI8gE8kaoI2ZtaY4gYwDrim7gJk1c85tj9wdBnzgb4jhENg/nBKBeEznpyQHcy64niAzGwz8geLpv085535tZvcCec65hWZ2H8UJ5DCwB/iBc+4fla0zKyvL5eXleR16uGmMRMLixNZqv37Fv/UdCR0zW+ucy6rOawMdI3HOLQGWnPDYXWVu3wnc6XdcIiISvbAPtkt1BHW0F9ajTLWUgqOJGymhRtABiIhIYlOLRJKXZpOFh7Z5UlOLREREYqIWiQTDj9aB+udTkup3+U8tEhFJGiXlhJ55cwu35bzDyxt3BB1SSlCLJIyS+Qg6iHGLZNyOUi7V7wqGWiQikjT6tGlC3fQ0ANXv8pFaJGES1llG8YxD4xbiIdXvCoYSSZQ0gCeSGFS/y3+B1tryQot2F7hbp/6Z/+7ZmxpmcVln2euB1E1P8/4CPGE5WledJJGUEUutraQbIzl09CiP563mP198gYOHD8dlneUN4FXk5Y07uGvBBs0WkcTWv3/l16UXKSPpurbOqd+A/+jVh9++sZLtRYU8MWQ4jevWi2md0V4PJG6XDg3LEb/GMyRg6lJODJUmEjNrADRxzm0+4fGOzrn3PI0sBhO79aB5/Qz+++W/MWpmDn8ZPpJWDRtVe33RDuBp6qEkvBBN+NA13RNHhV1bZnY18A9gjpm9b2bdyzz9tNeBxerKtufz3Igx7D14gFEzp7F2+7ZTv6gS2e2bcu/wDpV+kZN26mFurloj4ruqdClLsCobI/kp0M051xn4PvCsmY2IPBefUWyPZZ19DnOuvoaMOnW5du4sXvjoQ0/fr6Tlcl3Pljp6ksRUctDQr1/xT4AHEUl7YJaEKuvaSiu5zK1zbrWZXQosNrNzgYSZ6tWqYSPmjBnPxBcWcOuLi8kvLGBC1+5YnGZ0nSihph5q7ENCTOeEJI7KEkmhmX2rZHzEObfdzPoD84EL/QguXhrVrcuzV43m9lde5LdvrOSfBQX8ov/l1KyRZJPWlBgkXkLyHUqoA7MUVlki+QFQw8zaO+c2AjjnCs1sIDDOl+jiqHbNmvxhwJWc2yCDx/NWs23vXh4ZNIT6tWsHHZr/QjSgKiKJr8JE4px7F8DMNpjZs8ADQJ3I7yzgWV8ijKMaZtx+SR9aNMjg/159hbFzZjB16Aia1a8fdGixUWIQkQBFcx7JxcBvgVVAfeB5oJeXQXltbIeONKvfgFuWLGLkzGlMHTaC9k3ODDos/4Th/BAlO5GkEc0gwSFgP1CX4hbJp865o55G5YO+LVsxY8w4ahiMnT2d3M8+DTqk6gvRTBtJATrrXU4QTSJZQ3Ei6Q70Acab2SxPo/JJuzOaMOfqa2iZ0ZD/WDSP59e/G3RI/goi4ZTshFasKP7RTkkk4UXTtXWjcy4vcns7MNzMvudhTL466/T6zBg9jltfXMzPX32FrQVf8T+9+sat4KOv1AoRL2ksTipwykRSJomUfSzhBtorc1qtWkwZchW/WLGcKX/PY+vevTx0xUDq1EwPOrTkE4bxGRGJq6Qr2lhdNWvU4N7+l9MyoyH3vb6CHUWFPDHkKjLrxVbwUSRp6CBAKpBkZ+TFxsy4qWsWjw4eyvu7djFq5jQ++XJP0GElJ00IEEkaSiTlGPTttkwbOYavD33DqJk5rN6WH3RIIuGRYgcBusbQqSmRVKBLs7OZc/U1ZNary3XzZrPgww+CDklEqqm6yaCklP0zb27htpx3lEwqoERSiRYZDZk9ZjxdmjXjxy8t4bE1b5FslyYWSXaxJAOVso+OEskpNKxTl6eHj+Kq89vx0Jtv8L/LXuLQkSNBhyUiUYolGaiUfXQ0aysKtWvW5KErBnFuRgaPrH6LfxUW8qfBw2iQigUfRRJMtJfKLo9K2UfHkq2rJisry+XlnXTqS9zM3riBny5/mfMaNmLq8JGcU79BlV4f+mtQV2dqp6aDSsiF/v8uBMxsrXMuqzqvVddWFY1u34G/DB/J9qIiRs6Yxoad0fe3auBOJBjRXCpbqk+JpBp6nduSWWPGkZ5Wg7Gzp7Psk81RvS6mgTuva1JVpwaW6maFm/4e4pNAE4mZDTSzD83sYzO7o5zna5vZjMjzb5tZK/+jLF/bzDOYd/W1fLtxJhNfWMBf3/37KV+jgTsR/+j8D/8ENkZiZmnAR0A2kE9xleHxJVdjjCxzM9DROTfJzMYBI5xzYytbr9djJCfad+gQP37pBV7+ZDPf79yVn/buR1oll/Ctcl/tiYXy+vUr/u3VeITXYyQaT/Ge39+ZECrpRi4ZYH94fBd1a51Coo6R9AA+ds594pz7BpgODD9hmeHAXyO3ZwOXm4WrLG+99HT+NHgYN3Tuyl/W/Z0fLlnE/kOHKlxefbUi3tP5H/4KcvrvOcDWMvfzKb4aY7nLOOcOm1kBkAl8UXYhM5sATABo0aKFV/FWKK1GDe7qeynnNsjgV6+9yvi5M/nz0KtoUu+02Ffud6G86qy/Ki0RlSD3ng/fmbDPgoplyq9UXVIMtjvnpjjnspxzWU2aBPeF+X7nrky+cjgf7f6CUTOn8fGe3YHFIuKVRJh9WHL+x3U9W6pbywdBtki2AeeWud888lh5y+SbWU0gAwj13jn7W98mZ9RY/mPRPEbPyuHxwcPoeW4cWkmJfuSuEuT+82gbl9dtFMYddXb7pqGMKxkF2SJZA7Qxs9ZmVgsYByw8YZmFwPWR26OB5S4BzqDs1PQs5l59DWfWO40bFsxh7gfvBx2SLzRLJjVo9qGcKNAz281sMPAHIA14yjn3azO7F8hzzi00szrAs0AXYA8wzjn3SWXr9HvWVmX2HjzAD15YxJv5/+S2Hj350cU9qepcgbD3RZfQLJnUkijfS4leLLO2Aq215ZxbAiw54bG7ytw+AIzxO654aVC7Dt9t24sdXx7m4dVvsnVvAfddfgW10tKien3ZnfOsvPxQ75wTpbtD4iPIbiMlsfBJisH2sHp54w7+a8Z7fLypPmlfN2HePzZyw/w5FBw4ENXrE2kKo7o7xA+JMNCfipRIPHQsERiHCjK5JPMi1m7fxphZOWwtKDjl6xNp56xZMuKHRDq4SiVKJB46MRFc36UTf71qNDv3fc3ImdN49/Ptlb4+0XbOOtlSvJZIB1epRGXkPVZef+7He3Zz48J57Nr3Nb8fMJgB32oTWCwiMQlgOre+x96IZbBdiSQgX+zbx4RF83l3x3bu7N2PG7t0q/KMrqrQrCrxhM4LShqJWmsrpZ1Rrx7PjxzDFd9qw29eX8E9K5Zz+OhRz95PfcunoJLrVaNLCEgZSiQBqpuezmODh3JTl248+946Ji1ewNfffOPJe6lvWUS8oq6tkHj2vXX8YsVy2p/RhCeHjeDM006P+3skbd9yLN0rqVJy3asuqKquV11hoZWwJyTKMd/r2Jlz6jfgthcXM2LGNKYOG8EFZ8S31eDJSWTaMYikPLVIQub9nTu4cdE89h06xGODh9KnRaugQ6pctInEi4QTz9ZEsibEsLS4whKHVEiD7QmubLHDC89sytyrr+Gc+g24ceE8Zr6/PujwyqfBVgkRFQwNllokAatoWm7hwYPc8rdFrPznFn7Y/WL+6996eTo9uMqiPcL040g00VsTfsQflm3kQRya2h4fapEksIqm5davXZsnh45g7IUX8diat/nx0iUcPHw4yFCPl5tb/NOvX/FPyX1JHSFphWpqe/A02B6wyi4Jmp6Wxm8uy6ZFRgYPrnqdzwuLmDxkGA3r1A0w4iry44JWiZrA/Lz8cFi2kQdx6LK6wVMi8Ui0U21L6mlVtKyZ8YOsi2neIIPbl77IqJk5PDVsJC0bNgw8diA8Oyjxj58JMAqn+h8S72mMxANe9dmu3pbPpBcWUANjytCr6Nrs7DhEy3E7glD1N4elX99rifY5NQMrKWmMJGS86rPtcU5zZo8ZT/3atbl27iyWbPooLustS/3NckoaH5MTqGvLA1722Z7XqDGzx4xn4uL53PK3RdxZ2JebumRVb0ZXOV0UfRp9i1kdxgTb3xyyrhPPJevnkpShROIBr/tsM+vV47mRY/jJ0he57/XX2FJQwD39LqNmjZMbmFUti5L95Wb1N0t0lAAlQmMkCeyoczywaiVT1q6hf6vWPDJwCKfVqlX6fNTjHWE94g9rXCJJSGMkKaqGGXf06suvLv0OK7d8xtjZ0/m8qLD0eY13iIgflEiSwDUXdeLPQ0ewpeArRs2cxgdfFCeMqEvHh3WwNKxxichx1LWVRD7YtZMbF86j6JtveHTwUPq2bJW8peNFJK50qd0yUjmRAGwvLOSmRfP4aPcX/PLS7zCuQ8egQxKRBKAxEinVrH59ZoweR+8WLfnp8pd5cNVKjibYwYIquUp59L0ILyWSJHR6rVr8eegIxnfoyON5q/nPF18IV8HHSpTMNHvmzS3clvPOsZ1GSAoESjAq/F5IKCiRJKmaNWrwq0u/w/9c0ofFmz7ku/NmsWf/vqDDOiXPZ5opISUkzUAMNyWSJGZmTMrqwSMDh7B+5w5Gzczh06++DDqsSp000+zp3yf2BbQSLd6QinoGogRCZ7YngFhnXl3Z9nzOqn86ExbNZ/TMaTwx5Cqyzj4n7u8TDydVBbj5/visONXKrkSE4W8aD6rwG26atRVy8azG+9lXX/L9BXPZXlTIQ9mDuLLt+Z68jydi3fH7XbE2BBVyQ/83lVDRrK0kFs++4VYNGzHn6vFcdGZTbn1xMU+sXU3JgUTS90GnYMXapP+bSmgokYRcvPuGG9etx3MjxjCkzfn89o2V/N+rr3D46NHQ9kGXTvn804zE2vGHIHGF9W8qyUddWwnAi37uo87xu1WvM3ntavq1bMUjg4by5sdfhqoPOim6ZgIei0mWMRLxns5sLyMZE4mXpm94j5+/+gptM8/gyaEjaFa/ftAhlbprwQaeeXNL6f3rerbk3uEdAoxIJHkl3BiJmTU2s5fNbFPkd6MKljtiZusiPwv9jjMVjOvQkanDRrK1oKC44OOunUGHVEpdMyKJIZAWiZk9AOxxzt1vZncAjZxz/1vOckXOudOrsm61SKqnpOBj4TcHeXTQUPq1ah10SIC6ZkT8knBdW2b2IdDfObfdzJoBuc6588tZTonER58XFXLTwnl8qIKP5UvGc0+S8TNJtSRc1xbQ1Dm3PXL7c6CiQ806ZpZnZm+Z2VUVrczMJkSWy9u1S1Mcq+us0+szvUzBxwfeSLyCj3ICv8+sj/P7qVBjYvDszHYzewU4q5ynflb2jnPOmVlFe6uWzrltZnYesNzM1jvnNp+4kHNuCjAFilskMYae0koKPt6du4zJa1eTv7eAB7MHUrtmChdBSMaz4hPgM5WdtTcrLz8xZ+2lCM/2Ds6571T0nJntMLNmZbq2yh3hdc5ti/z+xMxygS7ASYlE4quk4GPLjIbc/8ZrbC8q5Ikhw2lct17QoUm0/E4UHrxfeSdUKpGEU1CHmQuB64H7I78XnLhAZCbXPufcQTM7A+gFPOBrlCnMzJjQrTvNGzTgv5b+jVEzc3hq+EhaNyx3gl1yK9kZhuWoPR5xhO0zRZSdXNGnTRNm5eWXnkekWXvhFVQiuR+YaWY3AluAqwHMLAuY5Jy7CWgHPGFmRykey7nfOdMZtRQAAAqHSURBVLcxoHhT1uA259P09NOZuGgBo2dOY8rQq+jW7OSCjxIyfieKOLxfeV1ZKtSYGAJJJM653cDl5TyeB9wUub0KuMjn0KQc3Zqdw+yrx/PvC+dx7dxZ/L8rBjG4zUmT7ALn+VThyM7RzynJx73XzWOLH4xnd1VIWiJQflfWvcM7KIEkANXakqi0atiIOWOKCz7e8rfjCz6GgV9X0Dvl+8Rx1tJJ79XoW1Vfid81vmJ4P52AmriUSCRqjerWLbfgYxj4VenWz4q6J73XDT+OayHIsE2tLbnmyHU9W2qGVoJJ4TmdUh21a9bkDwOvpHmDDCavXc2/CvfyyKChnF6rVqBx+TUwW+H7eDBrycvPFNaptdntm4YiDqkaJRKpshpm/E+vPrTIyODnr77CuNnTeXLYCM46PbiCj35dQc/PK/VV+F5x6KrS1FqJJ1X/lZi8tuUzfrhkIQ1q1+bJYSNpd0b5R80pUzMrZNNpK5IUJfolrhKxRIokib4tWzFz9Dicg7GzprNyy2cnLePXQLhET+MREk9KJBKzdk3OZM7V13BuRgb/vnAuM95ff9zzKXXJ1wS6hG92+6aaXitxoUQicdGsfn2mjxpLr3NbcueypTz05uul04M1rVMkuWmMROLq0JEj3J27jOnvr2fY+Rfw28sHULtmzdQZIxFJULGMkWjWlsRVeloav74smxYZDXlg1Uq2FxYy+crhmtYpksTUtSVxZ2ZMyurBwwOv5N3PP2f0rBy2fPVV0GGJiEeUSMQzQ9pewLMjR/Plgf2MnjWNd7b/K+iQRMQDSiTiqe5nN2f2mPGcXqs218ydxYsfbwo6JBGJMyUS8dx5jRoze8x4LmzShB8uWcjUd9aGquCjVC5sNbkkfJRIxBeZ9erx3MgxDPx2W369Mpd7ViwPTcFHqZhOJpVoKJGIb+rUTOeRQUOY0DWLZ99bx6TFC9h36FDQYZ1ER+DHpNTJpFJtSiTiqxpm3NG7H/f2v5zcLZ8ybs4Mdn5dFHRYpfw+Ag970tLJpBINJRIJxHc7dmbKkKv45Ms9jJw5jY92fxF0SIC/R+CJ0G2kmlwSDSUSCcxlrc8jZ9RYDh05yphZ01m19Z9Bh+TrEXiidBupJpecihKJBOqiM5sy9+praHb66dywYA5zPng/0Hj8PAJXt5EkC9XaklDYe/AgNy9ZyKqt/+RHF/fkth49MbOgw/KcapBJWMRSa0uJRELjmyNH+Nnyl5nzwfuMvKA9v7n8CmqlpQUdlkhKUNFGSQq10tJ44DsDaJGRwe/fWsX2oiIev3IoDWrXCTo0EamExkgkVMyMW3v05KHsQeT9K58xs6azbe/eoMMSkUookUgojWjXnqeHj+LzoiJGzpzG+p3hmxorIsWUSCS0ep7bgtljxlMrLY1xs6ez7NPNQYckIuVQIpFQa5OZydyrr+FbjTOZuHgBz763LuiQROQESiQSek1OO43po8ZyaavW3J27jN+szOVoks02FElkSiSSEOqlpzP5yuFc17EzT76zlluWLOLA4fAVfBRJRUokkjDSatTg7n6X8dPe/Xhp8ya+O3cWu/ftCzoskZSnRCIJxcy4qWsWjw4eyvu7djF6Vg6ffvVl0GGJpDQlEklIg77dludHjqHw4EFGz5xG3r+2BR2SSMpSIpGE1bXZ2cy+ejwZdery3XmzeOGjD4MOSSQlKZFIQmvVsBFzxoynY9OzuPXFxTyxdrWuBy/iMyUSSXiN6tbl2atGc2Wbtvz2jZXclbtM14MX8VEgicTMxpjZ+2Z21MwqrDZpZgPN7EMz+9jM7vAzRkkstWvW5I8DhzChW3eeX/8uExcv4Otvvgk6LJGUEFSLZAMwEnitogXMLA14DBgEtAfGm1l7f8KTRFTDjDt69eWXl36HFZHrwe8oCs/14EWSVSCJxDn3gXPuVCOjPYCPnXOfOOe+AaYDw72PThLdtRd14s9Dr+LTr75k/ocbgw5HJOmF+Xok5wBby9zPBy4ub0EzmwBMiNw9aGYbPI4tUZwBfBF0EEGaFPlB26IsbYtjtC2OOb+6L/QskZjZK8BZ5Tz1M+fcgni+l3NuCjAl8r551b3KV7LRtjhG2+IYbYtjtC2OMbNqX1rWs0TinPtOjKvYBpxb5n7zyGMiIhIiYZ7+uwZoY2atzawWMA5YGHBMIiJygqCm/44ws3ygJ/CCmb0UefxsM1sC4Jw7DNwCvAR8AMx0zr0fxeqneBR2ItK2OEbb4hhti2O0LY6p9rYwnQUsIiKxCHPXloiIJAAlEhERiUnCJxKVWznGzBqb2ctmtinyu1EFyx0xs3WRn6SawHCqv7OZ1TazGZHn3zazVv5H6Y8otsUNZrarzHfhpiDi9JqZPWVmOys6v8yKPRzZTu+ZWVe/Y/RLFNuiv5kVlPlO3BXNehM+kaByK2XdASxzzrUBlkXul2e/c65z5GeYf+F5K8q/843Al865bwO/B37rb5T+qMJ3fkaZ78KTvgbpn6eBgZU8PwhoE/mZADzuQ0xBeZrKtwXAyjLfiXujWWnCJxKVWznOcOCvkdt/Ba4KMJYgRPN3LruNZgOXm5n5GKNfUuU7f0rOudeAPZUsMhx4xhV7C2hoZs38ic5fUWyLakn4RBKl8sqtnBNQLF5q6pzbHrn9OdC0guXqmFmemb1lZsmUbKL5O5cuE5liXgBk+hKdv6L9zo+KdOfMNrNzy3k+FaTK/iFaPc3sXTP7m5ldGM0Lwlxrq5Sf5VbCrrJtUfaOc86ZWUVzu1s657aZ2XnAcjNb75zbHO9YJfQWATnOuYNmNpHiltplAcckwfo7xfuHIjMbDMynuMuvUgmRSFRu5ZjKtoWZ7TCzZs657ZGm+c4K1rEt8vsTM8sFugDJkEii+TuXLJNvZjWBDGC3P+H56pTbwjlX9nM/CTzgQ1xhlDT7h1g55/aWub3EzP5kZmc45yotbJkqXVupUm5lIXB95Pb1wEmtNTNrZGa1I7fPAHoByVJrPZq/c9ltNBpY7pLzrNxTbosTxgGGUVxBIhUtBK6LzN76N6CgTBdxSjGzs0rGDM2sB8U54tQHWs65hP4BRlDcp3kQ2AG8FHn8bGBJmeUGAx9RfOT9s6Dj9mhbZFI8W2sT8ArQOPJ4FvBk5PYlwHrg3cjvG4OOO87b4KS/M3AvMCxyuw4wC/gYWA2cF3TMAW6L+4D3I9+FV4ELgo7Zo+2QA2wHDkX2FTcSucJA5HmjeIbb5sj/RFbQMQe4LW4p8514C7gkmvWqRIqIiMQkVbq2RETEI0okIiISEyUSERGJiRKJiIjERIlERERiokQi4iMze9HMvjKzxUHHIhIvSiQi/noQ+F7QQYjEkxKJiAfMrHukGGIdMzstcs2cDs65ZUBh0PGJxFNC1NoSSTTOuTWRi4b9CqgLPOecK/diQiKJTolExDv3Ulzz6gBwW8CxiHhGXVsi3skETgfqU1zjSyQpKZGIeOcJ4OfA8yTpJX1FQF1bIp4ws+uAQ865aZHrp68ys8uAXwAXAKebWT7F1ZdfCjJWkVip+q+IiMREXVsiIhITJRIREYmJEomIiMREiURERGKiRCIiIjFRIhERkZgokYiISEz+P7EexDcFczRgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yWA2US_v_QEK",
        "outputId": "4078453e-6eca-4c77-91a9-574475422e9d"
      },
      "source": [
        "num_epochs = 100\n",
        "lambdaRate = 0.0\n",
        "alphaRate = 1.0\n",
        "\n",
        "costsList = []\n",
        "num_training = 100 # Maximum is 118\n",
        "num_validation = len(training_data) - num_training\n",
        "\n",
        "print(\"Network shape: s_l = \", end='')\n",
        "print(s_l)\n",
        "print(\"Training sets used: \" + str(num_training))\n",
        "print(\"Alpha rate: \" + str(alphaRate) + \" Lambda rate: \" + str(lambdaRate))\n",
        "print(\"Training epochs: \" + str(num_epochs), end=\"\\n\\n\")\n",
        "\n",
        "# Actual training starts here\n",
        "print(\"Initial weights\")\n",
        "for s in range(len(w)):\n",
        "    print(\"w^(%s) \" % (s+2))\n",
        "    print(w[s])\n",
        "\n",
        "# Train num_epoch times using the training set\n",
        "for epoch in range(num_epochs):\n",
        "    Delta = initializeDelta(s_l)\n",
        "    cost = 0\n",
        "    for m in range(int(num_validation/2), int(num_training+(num_validation/2))):\n",
        "        x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "        t = np.array([[float(training_data[m][2])]])\n",
        "        z, y = fowardPropagate(x, w, s_l)\n",
        "        cost += calculateSumError(y[-1], t, s_l)\n",
        "        delta = backPropagateError(t, y, w, s_l) \n",
        "        Delta = updateDelta(x, Delta, y, delta, s_l)\n",
        "    cost = cost/num_training\n",
        "    D = calculateWeightAdjust(Delta, w, s_l, num_training, lambdaRate)\n",
        "    for s in range(len(w)):\n",
        "        w[s] = np.copy(w[s] - alphaRate*D[s])\n",
        "    costsList.append(cost)\n",
        "\n",
        "# Caculate the cost one more time for final epoch\n",
        "cost = 0\n",
        "for m in range(int(num_validation/2), int(num_training+(num_validation/2))):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    cost += calculateSumError(y[-1], t, s_l)\n",
        "cost = cost/num_training\n",
        "costsList.append(cost)\n",
        "\n",
        "print(\"\\nFinal weights after training\")\n",
        "for s in range(len(w)):\n",
        "    print(\"w^(%s) \" % (s+2))\n",
        "    print(w[s])\n",
        "\n",
        "print(\"\\nCosts for all training epochs\")\n",
        "print(costsList)\n",
        "\n",
        "# Validation\n",
        "print(\"Validation test\")\n",
        "hits = 0.0\n",
        "for m in range(int(num_validation/2)):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    if (abs(t[0,0]-y[1][0,0]) <= 0.5):\n",
        "        hits += 1.0\n",
        "    print(\"Training data[\" + str(m) + \"]: Network output: \" + str(y[-1][0,0]) + \" Expected output: \" + str(t[0,0]))\n",
        "for m in range(int(num_training+(num_validation/2)), int(len(training_data))):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    if (abs(t[0,0]-y[1][0,0]) <= 0.5):\n",
        "        hits += 1.0\n",
        "    print(\"Training data[\" + str(m) + \"]: Network output: \" + str(y[-1][0,0]) + \" Expected output: \" + str(t[0,0]))\n",
        "accuracy = hits / num_validation\n",
        "print(\"Model accuracy: \" + str(accuracy))\n",
        "\n",
        "# Plotando fronteira de decisão\n",
        "x1s = np.linspace(-1,1.5,200)\n",
        "x2s = np.linspace(-1,1.5,200)\n",
        "zs=np.zeros((len(x1s),len(x2s)))\n",
        "\n",
        "for i in range(len(x1s)):\n",
        "    for j in range(len(x2s)):\n",
        "        xs = np.array([[x1s[i]],[x2s[j]]])\n",
        "        zss, yss = fowardPropagate(xs, w, s_l)\n",
        "        zs[i,j] = zss[-1][0,0] # saida do modelo antes de aplicar a função sigmoide \n",
        "plt.contour(x1s,x2s,np.transpose(zs),0)\n",
        "\n",
        "df=pd.read_csv(\"classification2.txt\", header=None)\n",
        "\n",
        "ins=df.iloc[:,:-1].values\n",
        "outs=df.iloc[:,-1].values\n",
        "pos , neg = (outs==1).reshape(118,1) , (outs==0).reshape(118,1)\n",
        "plt.scatter(ins[pos[:,0],0],ins[pos[:,0],1],c=\"r\",marker=\"+\")\n",
        "plt.scatter(ins[neg[:,0],0],ins[neg[:,0],1],marker=\"o\",s=10)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.legend([\"Accepted\",\"Rejected\"],loc=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network shape: s_l = [2, 3, 3, 1]\n",
            "Training sets used: 100\n",
            "Alpha rate: 1.0 Lambda rate: 0.0\n",
            "Training epochs: 100\n",
            "\n",
            "Initial weights\n",
            "w^(2) \n",
            "[[0.62683569 0.75802    0.46484621]\n",
            " [0.65330586 0.53143872 0.058914  ]\n",
            " [0.20692916 0.51821792 0.78195653]]\n",
            "w^(3) \n",
            "[[ 0.79863793  0.66006281 -0.02730273  0.35212904]\n",
            " [ 0.6894584   0.55156146  0.55914488  0.36325528]\n",
            " [ 0.18035378  0.29482936  0.91732882  0.70016568]]\n",
            "w^(4) \n",
            "[[-0.08870878  0.36470146 -0.12562009 -0.16664884]]\n",
            "\n",
            "Final weights after training\n",
            "w^(2) \n",
            "[[0.62814554 0.75662069 0.46637834]\n",
            " [0.65152295 0.53440482 0.05689446]\n",
            " [0.20637679 0.51853517 0.78133146]]\n",
            "w^(3) \n",
            "[[ 0.8013996   0.6589997  -0.02882721  0.35204467]\n",
            " [ 0.68818462  0.5514592   0.55925014  0.36288534]\n",
            " [ 0.17803473  0.29434282  0.91722418  0.69926472]]\n",
            "w^(4) \n",
            "[[-0.08161129  0.36157076 -0.13104659 -0.17712583]]\n",
            "\n",
            "Costs for all training epochs\n",
            "[0.12494262758382557, 0.12494188634472353, 0.12494139053835132, 0.12494105642778486, 0.12494082884956795, 0.12494067146173993, 0.12494056032094743, 0.12494047965185376, 0.1249404190603606, 0.12494037169778091, 0.12494033305139823, 0.124940300147647, 0.12494027102712335, 0.12494024439867743, 0.12494021941149634, 0.12494019550492486, 0.12494017230950798, 0.12494014958178465, 0.12494012716131862, 0.1249401049423855, 0.12494008285531333, 0.12494006085418909, 0.1249400389087553, 0.12494001699907185, 0.12493999511199735, 0.12493997323887063, 0.12493995137398262, 0.12493992951357001, 0.1249399076551517, 0.1249398857970911, 0.1249398639383085, 0.12493984207809108, 0.12493982021596756, 0.12493979835162632, 0.12493977648486024, 0.12493975461553176, 0.12493973274354865, 0.12493971086884882, 0.1249396889913898, 0.1249396671111424, 0.12493964522808562, 0.12493962334220471, 0.12493960145348815, 0.12493957956192688, 0.12493955766751373, 0.12493953577024251, 0.12493951387010754, 0.12493949196710387, 0.1249394700612265, 0.12493944815247104, 0.12493942624083301, 0.12493940432630786, 0.12493938240889149, 0.12493936048857948, 0.12493933856536751, 0.12493931663925163, 0.1249392947102272, 0.12493927277829027, 0.12493925084343653, 0.12493922890566175, 0.12493920696496169, 0.12493918502133214, 0.12493916307476885, 0.12493914112526766, 0.12493911917282417, 0.12493909721743435, 0.12493907525909387, 0.12493905329779854, 0.12493903133354406, 0.1249390093663263, 0.12493898739614087, 0.12493896542298372, 0.12493894344685039, 0.12493892146773695, 0.12493889948563884, 0.12493887750055206, 0.12493885551247219, 0.12493883352139507, 0.12493881152731642, 0.12493878953023203, 0.12493876753013768, 0.1249387455270292, 0.12493872352090214, 0.12493870151175225, 0.1249386794995755, 0.12493865748436755, 0.12493863546612392, 0.12493861344484065, 0.12493859142051335, 0.12493856939313785, 0.12493854736270985, 0.12493852532922496, 0.12493850329267916, 0.12493848125306801, 0.12493845921038726, 0.12493843716463275, 0.1249384151158001, 0.12493839306388521, 0.12493837100888364, 0.1249383489507912, 0.12493832688960352]\n",
            "Validation test\n",
            "Training data[0]: Network output: 0.48966234941083775 Expected output: 1.0\n",
            "Training data[1]: Network output: 0.4897384414112005 Expected output: 1.0\n",
            "Training data[2]: Network output: 0.4898062992998899 Expected output: 1.0\n",
            "Training data[3]: Network output: 0.489889434987222 Expected output: 1.0\n",
            "Training data[4]: Network output: 0.4899729816361088 Expected output: 1.0\n",
            "Training data[5]: Network output: 0.4899680526252268 Expected output: 1.0\n",
            "Training data[6]: Network output: 0.48987823237419525 Expected output: 1.0\n",
            "Training data[7]: Network output: 0.48980778151358007 Expected output: 1.0\n",
            "Training data[8]: Network output: 0.4896069734765036 Expected output: 1.0\n",
            "Training data[109]: Network output: 0.4900425892059783 Expected output: 0.0\n",
            "Training data[110]: Network output: 0.48998728752307597 Expected output: 0.0\n",
            "Training data[111]: Network output: 0.49009338385118295 Expected output: 0.0\n",
            "Training data[112]: Network output: 0.4901805927698488 Expected output: 0.0\n",
            "Training data[113]: Network output: 0.4901119565633546 Expected output: 0.0\n",
            "Training data[114]: Network output: 0.49002612151054187 Expected output: 0.0\n",
            "Training data[115]: Network output: 0.4899842556219238 Expected output: 0.0\n",
            "Training data[116]: Network output: 0.48971229234739083 Expected output: 0.0\n",
            "Training data[117]: Network output: 0.48933976674567703 Expected output: 0.0\n",
            "Model accuracy: 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7feee43c9810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}