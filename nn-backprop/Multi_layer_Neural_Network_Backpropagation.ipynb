{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-layer_Neural_Network_Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVMpLxf4Z-3L"
      },
      "source": [
        "# Multi-layer Neural Newtork with Backpropagation\n",
        "### Vinicius Pimenta Bernardo (202002447)\n",
        "### Thiago Werneck Ferreira dos Santos (202003651)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc-JvCZGZ8oL"
      },
      "source": [
        "# Import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm1ogXIxZ793"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List\n",
        "\n",
        "from decimal import *\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZDmdhJTYq7b"
      },
      "source": [
        "getcontext().prec = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0qEKaeKuGwH"
      },
      "source": [
        "$L$ is the number of layers, including the input and output layers, and $s_L^{(l)}$ is the number of neurons per layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOlPACq8QzCA"
      },
      "source": [
        "# Upload training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "deo9G_CJZ-BL",
        "outputId": "c795fe72-fbc7-4217-a3a8-ce7a744e26a1"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# Upload test_data.txt, needs to be repeated with every new session"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-83a42825-40e2-4943-a5f2-2657d6387cf7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-83a42825-40e2-4943-a5f2-2657d6387cf7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_data.txt to test_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlB8v4c8aMbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66700714-6ac9-4e7d-c1ef-f6c191ae7bb0"
      },
      "source": [
        "file_name = \"test_data.txt\"\n",
        "training_data = uploaded[file_name].decode(\"utf-8\").split(\"\\n\")\n",
        "# Parse data\n",
        "for i in range(len(training_data)):\n",
        "    training_data[i] = training_data[i].split(\",\")\n",
        "print(training_data)\n",
        "print(len(training_data))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['-1.0', '-1.0', '0.0'], ['-0.9747474747474747', '-0.9747474747474747', '0.0'], ['-0.9494949494949495', '-0.9494949494949495', '0.0'], ['-0.9242424242424242', '-0.9242424242424242', '0.0'], ['-0.898989898989899', '-0.898989898989899', '0.0'], ['-0.8737373737373737', '-0.8737373737373737', '0.0'], ['-0.8484848484848485', '-0.8484848484848485', '0.0'], ['-0.8232323232323232', '-0.8232323232323232', '0.0'], ['-0.797979797979798', '-0.797979797979798', '0.0'], ['-0.7727272727272727', '-0.7727272727272727', '0.0'], ['-0.7474747474747474', '-0.7474747474747474', '0.0'], ['-0.7222222222222222', '-0.7222222222222222', '0.0'], ['-0.696969696969697', '-0.696969696969697', '0.0'], ['-0.6717171717171717', '-0.6717171717171717', '0.0'], ['-0.6464646464646464', '-0.6464646464646464', '0.0'], ['-0.6212121212121212', '-0.6212121212121212', '0.0'], ['-0.595959595959596', '-0.595959595959596', '0.0'], ['-0.5707070707070707', '-0.5707070707070707', '0.0'], ['-0.5454545454545454', '-0.5454545454545454', '0.0'], ['-0.5202020202020202', '-0.5202020202020202', '0.0'], ['-0.4949494949494949', '-0.4949494949494949', '0.0'], ['-0.4696969696969697', '-0.4696969696969697', '0.0'], ['-0.4444444444444444', '-0.4444444444444444', '0.0'], ['-0.4191919191919192', '-0.4191919191919192', '0.0'], ['-0.3939393939393939', '-0.3939393939393939', '0.0'], ['-0.36868686868686873', '-0.36868686868686873', '0.0'], ['-0.3434343434343434', '-0.3434343434343434', '0.0'], ['-0.31818181818181823', '-0.31818181818181823', '0.0'], ['-0.29292929292929293', '-0.29292929292929293', '0.0'], ['-0.26767676767676774', '-0.26767676767676774', '0.0'], ['-0.24242424242424243', '-0.24242424242424243', '0.0'], ['-0.21717171717171724', '-0.21717171717171724', '0.0'], ['-0.19191919191919193', '-0.19191919191919193', '0.0'], ['-0.16666666666666663', '-0.16666666666666663', '0.0'], ['-0.14141414141414144', '-0.14141414141414144', '0.0'], ['-0.11616161616161613', '-0.11616161616161613', '0.0'], ['-0.09090909090909094', '-0.09090909090909094', '0.0'], ['-0.06565656565656564', '-0.06565656565656564', '0.0'], ['-0.04040404040404044', '-0.04040404040404044', '0.0'], ['-0.015151515151515138', '-0.015151515151515138', '0.0'], ['0.010101010101010166', '0.010101010101010166', '0.0'], ['0.03535353535353525', '0.03535353535353525', '0.0'], ['0.06060606060606055', '0.06060606060606055', '0.0'], ['0.08585858585858586', '0.08585858585858586', '0.0'], ['0.11111111111111116', '0.11111111111111116', '0.0'], ['0.13636363636363624', '0.13636363636363624', '0.0'], ['0.16161616161616155', '0.16161616161616155', '0.0'], ['0.18686868686868685', '0.18686868686868685', '0.0'], ['0.21212121212121215', '0.21212121212121215', '0.0'], ['0.23737373737373746', '0.23737373737373746', '0.0'], ['0.26262626262626254', '0.26262626262626254', '1.0'], ['0.28787878787878785', '0.28787878787878785', '1.0'], ['0.31313131313131315', '0.31313131313131315', '1.0'], ['0.33838383838383845', '0.33838383838383845', '1.0'], ['0.36363636363636354', '0.36363636363636354', '1.0'], ['0.38888888888888884', '0.38888888888888884', '1.0'], ['0.41414141414141414', '0.41414141414141414', '1.0'], ['0.43939393939393945', '0.43939393939393945', '1.0'], ['0.46464646464646453', '0.46464646464646453', '1.0'], ['0.48989898989898983', '0.48989898989898983', '1.0'], ['0.5151515151515151', '0.5151515151515151', '1.0'], ['0.5404040404040404', '0.5404040404040404', '1.0'], ['0.5656565656565655', '0.5656565656565655', '1.0'], ['0.5909090909090908', '0.5909090909090908', '1.0'], ['0.6161616161616161', '0.6161616161616161', '1.0'], ['0.6414141414141414', '0.6414141414141414', '1.0'], ['0.6666666666666667', '0.6666666666666667', '1.0'], ['0.6919191919191918', '0.6919191919191918', '1.0'], ['0.7171717171717171', '0.7171717171717171', '1.0'], ['0.7424242424242424', '0.7424242424242424', '1.0'], ['0.7676767676767677', '0.7676767676767677', '1.0'], ['0.7929292929292928', '0.7929292929292928', '1.0'], ['0.8181818181818181', '0.8181818181818181', '1.0'], ['0.8434343434343434', '0.8434343434343434', '1.0'], ['0.8686868686868687', '0.8686868686868687', '1.0'], ['0.8939393939393938', '0.8939393939393938', '1.0'], ['0.9191919191919191', '0.9191919191919191', '1.0'], ['0.9444444444444444', '0.9444444444444444', '1.0'], ['0.9696969696969697', '0.9696969696969697', '1.0'], ['0.9949494949494948', '0.9949494949494948', '1.0'], ['1.0202020202020203', '1.0202020202020203', '1.0'], ['1.0454545454545454', '1.0454545454545454', '1.0'], ['1.0707070707070705', '1.0707070707070705', '1.0'], ['1.095959595959596', '1.095959595959596', '1.0'], ['1.121212121212121', '1.121212121212121', '1.0'], ['1.1464646464646466', '1.1464646464646466', '1.0'], ['1.1717171717171717', '1.1717171717171717', '1.0'], ['1.1969696969696968', '1.1969696969696968', '1.0'], ['1.2222222222222223', '1.2222222222222223', '1.0'], ['1.2474747474747474', '1.2474747474747474', '1.0'], ['1.2727272727272725', '1.2727272727272725', '1.0'], ['1.297979797979798', '1.297979797979798', '1.0'], ['1.323232323232323', '1.323232323232323', '1.0'], ['1.3484848484848486', '1.3484848484848486', '1.0'], ['1.3737373737373737', '1.3737373737373737', '1.0'], ['1.3989898989898988', '1.3989898989898988', '1.0'], ['1.4242424242424243', '1.4242424242424243', '1.0'], ['1.4494949494949494', '1.4494949494949494', '1.0'], ['1.474747474747475', '1.474747474747475', '1.0'], ['1.5', '1.5', '1.0']]\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5ObSWr0ZcLI"
      },
      "source": [
        "reference for this section:\n",
        "\n",
        "https://buomsoo-kim.github.io/colab/2018/04/15/Colab-Importing-CSV-and-JSON-files-in-Google-Colab.md/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKrjEw9xQ3LQ"
      },
      "source": [
        "# Weights initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhaqysNuNtQj"
      },
      "source": [
        "There is a separate weight matrix for each layer $l \\in L,\\space l>1$ so, there will be a matrx $w^{(L)}, \\space w^{(L-1)}, \\space w^{(L-2)}, \\space ..., w^{(2)}$\n",
        "\n",
        "A matrix $w^{(l)}$ will be used to calculate the activation for that same layer $l$ using the activations from layer $l-1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hw5ZJPNyLL-"
      },
      "source": [
        "def initializeWeights(layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns a list of arrays containing random weights\n",
        "    \"\"\"\n",
        "    weights = []\n",
        "    for l in range(1, len(layers)):\n",
        "        weights.append(np.random.rand(layers[l], layers[l-1]+1))\n",
        "    return weights"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf_3RCV0ZyoA"
      },
      "source": [
        " # Foward propagation function definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnXB3xoWi3Et"
      },
      "source": [
        "$z^{(l)}_i=\\sum_{j=0}^{s_L^{(l-1)}}w_{ij}^{(l)}y_{j}^{(l-1)}$\n",
        "\n",
        "for a given $i$, $w_{ij}^{(l)}$ is the line ```[i:]``` from the weight matrix of layer $(l)$, $y_j^{(l-1)}$ is the column o activation values of layer $(l-1)$, and $s_L^{(n)}$ is the number of neurons on a given layer $n$\n",
        "\n",
        "$\\begin{pmatrix}\n",
        "  w^{(l)}_{i,0} & w^{(l)}_{i,1} & \\cdots & w^{(l)}_{i,s_L^{(l-1)}}\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "  1\\\\\n",
        "  y_{1}^{(l-1)}\\\\\n",
        "  \\vdots\\\\\n",
        "  y_{s_L^{(l-1)}}^{(l-1)}\n",
        "\\end{pmatrix}=z_i^{(l)}$\n",
        "\n",
        "and the activation $y_i^{(l)}$ is $g(z_i^{(l)})=\\frac{1}{1+e^{z_i^{(l)}}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGfqO8d0PJ6b"
      },
      "source": [
        "def transfer(weights: np.ndarray, activations: np.ndarray) ->float:\n",
        "    \"\"\"\n",
        "    Single neuron weighted sum\n",
        "    \"\"\"\n",
        "    transfer = weights[0] # bias weight * 1\n",
        "    transfer += np.matmul(weights[1:], activations)[0]\n",
        "    return transfer\n",
        "\n",
        "def activation(transfer: float) ->float:\n",
        "    \"\"\"\n",
        "    Single neuron activation function\n",
        "    \"\"\"\n",
        "    return (1/(1+math.exp(-transfer)))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhMflICxsKaG"
      },
      "source": [
        "compute $z^{(1)}$ and $y^{(1)}$ using $x^{(m)}$ for one $m$ corresponding to a single training set and then $z^{(l)}$ and $y^{(l)}$ using $y^{(l-1)}$ for $l\\in2...L$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H6sputTe6VJ"
      },
      "source": [
        "def fowardPropagate(inputs: np.ndarray, weights: List[np.ndarray], layers: List) ->(List[np.ndarray], List[np.ndarray]):\n",
        "    \"\"\"Propagate the input throughout the network\n",
        "\n",
        "    The second layer activation is calculated using the inputs, which are the activation of the first layer,\n",
        "    and the remaining layers activation values are calculated using the values from previous layer\n",
        "    @param inputs: A single set of inputs from the training set as a column\n",
        "    @param weights: The network neurons weights\n",
        "    @param layers: List containing number of units per layer\n",
        "    @return neuronsNetSum, neuronsActivation: The propagated transfer and activation values of neurons\n",
        "    \"\"\"\n",
        "    neuronsTransfer = []\n",
        "    neuronsActivation = []\n",
        "    layerTransfer = np.array([[transfer(w[0][i,:], inputs)] for i in range(layers[1])])\n",
        "    layerActivation = np.array([[activation(layerTransfer[i])] for i in range(layers[1])])\n",
        "    neuronsTransfer.append(layerTransfer)\n",
        "    neuronsActivation.append(layerActivation)\n",
        "    \n",
        "    for l in range(2, len(layers)):\n",
        "        layerTransfer = np.array([[transfer(w[l-1][i,:], neuronsActivation[l-2])] for i in range(layers[l])])\n",
        "        layerActivation = np.array([[activation(layerTransfer[i])] for i in range(layers[l])])\n",
        "        neuronsTransfer.append(layerTransfer)\n",
        "        neuronsActivation.append(layerActivation)\n",
        "\n",
        "    return neuronsTransfer, neuronsActivation"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYgj98XkDabf"
      },
      "source": [
        "# Error calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdUQNWVUTDKd"
      },
      "source": [
        "$J(w) = \\frac{1}{2}*\\sum_{i=1}^{s_L^{(L)}}(y_i^{(L)}-t_i)^2$\n",
        "\n",
        "Considering the error function as:\n",
        "\n",
        "$E = \\frac{1}{2}*(y_i^{(L)}-t_i)^2$\n",
        "\n",
        "for a single neuron $i$ on the output layer, and as:\n",
        "\n",
        "$E=\\frac{1}{2n}\\sum_{m=1}^n (y^{(L)}_i-t^{(m)}_i)$\n",
        "\n",
        "for $n$ training examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urah2l0WDiVG"
      },
      "source": [
        "def calculateSumError(outputs: np.ndarray, expected: np.ndarray, layers: List) ->float:\n",
        "    \"\"\"\n",
        "    Returns the accumulated error J(w) of the last layer\n",
        "    \"\"\"\n",
        "    sumError = 0.0\n",
        "    for i in range(layers[-1]):\n",
        "        sumError += ((outputs[i][0] - expected[i][0]) ** 2) * 0.5\n",
        "    return sumError"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaRHvCTIXv3x"
      },
      "source": [
        "# Backpropagatin function definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwmM-ZQafn-Y"
      },
      "source": [
        "Considering the error function as:\n",
        "\n",
        "$E = \\frac{1}{2}*(y_i^{(L)}-t_i)^2$\n",
        "\n",
        "for a single neuron $i$ on the output layer.\n",
        "\n",
        "The derivative of the error is:\n",
        "\n",
        "$\\frac{\\delta E}{\\delta w^{(l)}_{ij}} = \\frac{\\delta E}{\\delta y^{(l)}_i}\\frac{\\delta y^{(l)}_i}{\\delta z^{(l)}_i}\\frac{\\delta z^{(l)}_i}{\\delta w^{(l)}_{ij}}=\\frac{\\delta E}{\\delta y^{(l)}_i}\\frac{\\delta y^{(l)}_i}{\\delta z^{(l)}_i}y^{(l-1)}_j=\\delta^{(l)}_iy^{(l-1)}_j$\n",
        "\n",
        "$\\frac{\\delta y_i^{(l)}}{\\delta z_i^{(l)}} = y_i^{(l)}(1-y_i^{(l)})$ when $y_i^{(l)}$ is $g(z_i^{(l)})=\\frac{1}{1+e^{-z_i^{(l)}}}$\n",
        "\n",
        "$\\delta^{(l)}_j=\\frac{\\delta E}{\\delta y^{(l)}_j}\\frac{\\delta y^{(l)}_j}{\\delta z^{(l)}_j}=\\begin{cases}(y^{(l)}_j-t_j)y^{(l)}_j(1-y^{(l)}_j), \\space l=L \\\\ (\\sum_{i=1}^{s_L^{(l+1)}}w_{ij}^{(l+1)}\\delta_{i}^{(l+1)})y^{(l)}_j(1-y^{(l)}_j), \\space l<L\\end{cases}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl4F7CpyXtTt"
      },
      "source": [
        "def activationDerivate(activation: float) ->float:\n",
        "    \"\"\"\n",
        "    Single neuron activation derivate function\n",
        "    \"\"\"\n",
        "    return activation*(1.0 - activation)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2FS3W9mRgVV"
      },
      "source": [
        "def backPropagateError(outputs: np.ndarray, activations: List[np.ndarray], weights: List[np.ndarray], layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"Backpropagate the error throughout the network\n",
        "\n",
        "    The last layer error is calculated using the expected outputs from the training examples,\n",
        "    and the remaining layers error values are calculated using the values from upfront layer\n",
        "    @param outputs: A single set of outputs from the training set as a column\n",
        "    @param activation: Calculated using the foward propagation\n",
        "    @param weights: The network neurons weights\n",
        "    @param layers: List containing number of units per layer\n",
        "    @return neuronsErrorDerivatives: The backpropagated error values of neurons\n",
        "    \"\"\"\n",
        "    neuronsErrorDerivatives = []\n",
        "    layerErrorDerivative = np.array([[((activations[len(layers)-2][i,0] - outputs[i,0])*activationDerivate(activations[len(layers)-2][i,0]))] for i in range(layers[len(layers)-1])])\n",
        "    neuronsErrorDerivatives.append(layerErrorDerivative)\n",
        "\n",
        "    for l in reversed(range(1, len(layers)-1)):\n",
        "        layerErrorDerivative = np.matmul(np.transpose(weights[l][:,1:]), neuronsErrorDerivatives[0])*activationDerivate(activations[l-1])\n",
        "        neuronsErrorDerivatives.insert(0, layerErrorDerivative)\n",
        "\n",
        "    return neuronsErrorDerivatives"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsIhyhnR0pw9"
      },
      "source": [
        "# Update weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XskazuECS_S7"
      },
      "source": [
        "$\\Delta^{(l)}_{ij} = \\Delta^{(l)}_{ij}+y^{(l-1)}_j \\delta^{(l)}_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlN5vnDp4Jwx"
      },
      "source": [
        "def initializeDelta(layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns a list of arrays containing zeros\n",
        "    \"\"\"\n",
        "    Delta = []\n",
        "    for l in range(1, len(layers)):\n",
        "        Delta.append(np.zeros((layers[l], layers[l-1]+1)))\n",
        "    return Delta\n",
        "\n",
        "def updateDelta(inputs: np.ndarray, Delta: List[np.ndarray], activations: List[np.ndarray], delta: List[np.ndarray], layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns the updated value of Delta using delta and activation values\n",
        "    \"\"\"\n",
        "    updatedDelta = []\n",
        "    updatedDelta.append(Delta[0] + np.matmul(delta[0], np.array([np.append(np.array([[1]]), inputs)]) ) )\n",
        "    for l in range(2, len(layers)):\n",
        "        updatedDelta.append(Delta[l-1] + np.matmul(delta[l-1], np.array([np.append(np.array([[1]]), activations[l-2])]) ) )\n",
        "    return updatedDelta"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LriHK922lORh"
      },
      "source": [
        "$D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij}+\\lambda w^{(l)}_{ij}$ for $j\\neq 0$\n",
        "\n",
        "and\n",
        "\n",
        "$D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij}$ for $j=0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFg40anQdWEx"
      },
      "source": [
        "def regRateMatrix(regRate: float, layers: List) ->List[np.array]:\n",
        "    \"\"\"\n",
        "    Returns the regularization matrix with the first columns being 0.0 and lambda for the rest\n",
        "    \"\"\"\n",
        "    regRates = []\n",
        "    for l in range(1, len(layers)):\n",
        "        line = np.append(np.array([0.0]), np.linspace(regRate, regRate, layers[l-1]))\n",
        "        grid = np.array([line])\n",
        "        for _ in range(1, layers[l]):\n",
        "            grid = np.append(grid, [line], axis=0)\n",
        "        regRates.append(grid)\n",
        "\n",
        "    return regRates\n",
        "\n",
        "def calculateWeightAdjust(Delta: List[np.ndarray], weights: List[np.ndarray], layers: List, num_examples: float, regRate: float=0.0) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns matrixes containing the cost derivate for each weight\n",
        "    \"\"\"\n",
        "    weightsAdjust = []\n",
        "    regRates = regRateMatrix(regRate, layers)\n",
        "    for l in range(1, len(layers)):\n",
        "        layerWeightAdjust = Delta[l-1]*(1.0/num_examples) + weights[l-1]*regRates[l-1]\n",
        "        weightsAdjust.append(layerWeightAdjust)\n",
        "    \n",
        "    return weightsAdjust"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGkDU-G59PPB"
      },
      "source": [
        "# Approximate gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bws2fCLUtbj-"
      },
      "source": [
        "Same functions as before but using Decimal type instead of float (still did not solve problem)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMTNFym_9Tag"
      },
      "source": [
        "def initializeGradApprox(layers: List) ->List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns a list of arrays containing zeros\n",
        "    \"\"\"\n",
        "    gradApprox = []\n",
        "    for l in range(1, len(layers)):\n",
        "        gradApprox.append(np.zeros((layers[l], layers[l-1]+1), dtype=Decimal))\n",
        "    return gradApprox\n",
        "\n",
        "def transferNumerical(weights: np.ndarray, activations: np.ndarray) ->Decimal:\n",
        "    \"\"\"\n",
        "    Single neuron weighted sum\n",
        "    \"\"\"\n",
        "    transfer = Decimal(weights[0]) # bias weight * 1\n",
        "    transfer += Decimal(np.matmul(weights[1:], activations)[0])\n",
        "    return transfer\n",
        "\n",
        "def activationNumerical(transfer: Decimal) ->Decimal:\n",
        "    \"\"\"\n",
        "    Single neuron activation function\n",
        "    \"\"\"\n",
        "    return (Decimal(1.0)/Decimal((1.0+math.exp(-transfer))))\n",
        "\n",
        "def calculateSumErrorNumerical(outputs: np.ndarray, expected: np.ndarray, layers: List) ->Decimal:\n",
        "    sumError = Decimal(0.0)\n",
        "    for i in range(layers[-1]):\n",
        "        sumError += Decimal(((outputs[i][0] - expected[i][0]) ** 2)) * Decimal(0.5)\n",
        "    return sumError\n",
        "\n",
        "def activationDerivateNumerical(activation: float) ->Decimal:\n",
        "    \"\"\"\n",
        "    Single neuron activation derivate function\n",
        "    \"\"\"\n",
        "    return Decimal(activation)*(Decimal(1.0 - activation))\n",
        "\n",
        "def fowardPropagateNumerical(inputs: np.ndarray, weights: List[np.ndarray], layers: List) ->(List[np.ndarray], List[np.ndarray]):\n",
        "    \"\"\"Propagate the input throughout the network with Decimal instead of float\n",
        "\n",
        "    The second layer activation is calculated using the inputs, which are the activation of the first layer,\n",
        "    and the remaining layers activation values are calculated using the values from previous layer\n",
        "    @param inputs: A single set of inputs from the training set as a column\n",
        "    @param weights: The network neurons weights\n",
        "    @param layers: List containing number of units per layer\n",
        "    @return neuronsNetSum, neuronsActivation: The propagated transfer and activation values of neurons\n",
        "    \"\"\"\n",
        "    neuronsTransfer = []\n",
        "    neuronsActivation = []\n",
        "    layerTransfer = np.array([[transfer(w[0][i,:], inputs)] for i in range(layers[1])], dtype=Decimal)\n",
        "    layerActivation = np.array([[activation(layerTransfer[i])] for i in range(layers[1])], dtype=Decimal)\n",
        "    neuronsTransfer.append(layerTransfer)\n",
        "    neuronsActivation.append(layerActivation)\n",
        "    \n",
        "    for l in range(2, len(layers)):\n",
        "        layerTransfer = np.array([[transfer(w[l-1][i,:], neuronsActivation[l-2])] for i in range(layers[l])], dtype=Decimal)\n",
        "        layerActivation = np.array([[activation(layerTransfer[i])] for i in range(layers[l])], dtype=Decimal)\n",
        "        neuronsTransfer.append(layerTransfer)\n",
        "        neuronsActivation.append(layerActivation)\n",
        "\n",
        "    return neuronsTransfer, neuronsActivation\n",
        "\n",
        "def calculateGradApprox(weights: List[np.ndarray], inputs: np.ndarray, expected: np.ndarray, layers: List) ->List[np.ndarray]:\n",
        "    epsilon = 0.00001\n",
        "    gradApprox = []\n",
        "    for l in range(1, len(layers)):\n",
        "        layer_weights = np.zeros((layers[l], layers[l-1]+1), dtype=Decimal)\n",
        "        for i in range(0, layers[l]):\n",
        "            for j in range(0, layers[l-1]+1):\n",
        "                w_plus = np.copy(weights)\n",
        "                w_minus = np.copy(weights)\n",
        "                w_plus[l-1][i,j] += epsilon\n",
        "                w_minus[l-1][i,j] -= epsilon\n",
        "                z_plus, y_plus = fowardPropagateNumerical(inputs, w_plus, layers)\n",
        "                sumError_plus = calculateSumErrorNumerical(y_plus[-1], expected, layers)\n",
        "                z_minus, y_minus = fowardPropagateNumerical(inputs, w_minus, layers)\n",
        "                sumError_minus = calculateSumErrorNumerical(y_minus[-1], expected, layers)\n",
        "                dJ_dwij = (Decimal(sumError_plus) - Decimal(sumError_minus)) / Decimal((2.0 * epsilon))\n",
        "                layer_weights[i, j] += dJ_dwij\n",
        "        gradApprox.append(layer_weights)\n",
        "    return gradApprox"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwuzDWktDDmv"
      },
      "source": [
        "# Run training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1KvQ4iVqaWA"
      },
      "source": [
        "### Compare numerical and standard gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-vGbGdT5zAt",
        "outputId": "29f2641d-b7cc-408a-9c83-99b8898b4795"
      },
      "source": [
        "######################################################################\n",
        "# Run one epoch to compare numerical gradient with calculated derivate\n",
        "######################################################################\n",
        "# s_l always needs 2 at beginning and 1 at the end to comply with data format\n",
        "# s_l = [2, 1]\n",
        "s_l = [2, 3, 3, 1]\n",
        "initial_weights = initializeWeights(s_l)\n",
        "w = initial_weights.copy()\n",
        "lambdaRate = 0.0\n",
        "alphaRate = 1.0\n",
        "costsList = []\n",
        "\n",
        "num_training = len(training_data)\n",
        "# num_training = 100\n",
        "\n",
        "print(\"Network shape: s_l = \", end='')\n",
        "print(s_l)\n",
        "print(\"Training sets used: \" + str(num_training))\n",
        "print(\"Alpha rate: \" + str(alphaRate) + \" Lambda rate: \" + str(lambdaRate))\n",
        "print(\"\")\n",
        "\n",
        "print(\"Initial weights\")\n",
        "for s in range(len(w)):\n",
        "    print(\"w^(%s) \" % (s+2))\n",
        "    print(w[s])\n",
        "\n",
        "# Init stuff with 0\n",
        "Delta = initializeDelta(s_l)\n",
        "gradApproxSum = initializeGradApprox(s_l)\n",
        "cost = 0\n",
        "# Run one epoch\n",
        "for m in range(num_training):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    cost += calculateSumError(y[-1], t, s_l)\n",
        "    delta = backPropagateError(t, y, w, s_l) \n",
        "    Delta = updateDelta(x, Delta, y, delta, s_l)\n",
        "    gradApprox = calculateGradApprox(w, x, t, s_l)\n",
        "    for s in range(len(gradApprox)):\n",
        "        gradApproxSum[s] = gradApproxSum[s] + gradApprox[s]\n",
        "cost = cost/num_training\n",
        "costsList.append(cost)\n",
        "D = calculateWeightAdjust(Delta, w, s_l, num_training, lambdaRate)\n",
        "\n",
        "# Print both derivates (calculated and numerical)\n",
        "print(\"\\nCost derivates\")\n",
        "for s in range(len(D)):\n",
        "    print(\"D^(%s) \" % (s+2))\n",
        "    print(D[s])\n",
        "for s in range(len(gradApproxSum)):\n",
        "    gradApproxSum[s] = gradApproxSum[s] * Decimal((1.0/num_training))\n",
        "print(\"\\nApproximate derivates (numerical method)\")\n",
        "for s in range(len(gradApproxSum)):\n",
        "    print(\"D_approx^(%s) \" % (s+2))\n",
        "    print(gradApproxSum[s])\n",
        "\n",
        "# Adjust weights using derivate\n",
        "for s in range(len(w)):\n",
        "    w[s] = np.copy(w[s] - alphaRate * D[s])\n",
        "print(\"\\nFinal weights after training\")\n",
        "for s in range(len(w)):\n",
        "    print(\"w^(%s) \" % (s+2))\n",
        "    print(w[s])\n",
        "\n",
        "# Caculate the cost one more time for final weights\n",
        "cost = 0\n",
        "for m in range(num_training):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    cost += calculateSumError(y[-1], t, s_l)\n",
        "cost = cost/num_training\n",
        "costsList.append(cost)\n",
        "\n",
        "print(\"\\nCosts for all epochs\")\n",
        "print(costsList)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network shape: s_l = [2, 3, 3, 1]\n",
            "Training sets used: 100\n",
            "Alpha rate: 1.0 Lambda rate: 0.0\n",
            "\n",
            "Initial weights\n",
            "w^(2) \n",
            "[[0.60624675 0.72314269 0.14342055]\n",
            " [0.57784535 0.0505162  0.93774799]\n",
            " [0.93425864 0.46592751 0.88521617]]\n",
            "w^(3) \n",
            "[[0.57113819 0.97719776 0.23094991 0.31881499]\n",
            " [0.50958634 0.83900372 0.25831886 0.0358421 ]\n",
            " [0.58089538 0.50763607 0.65000069 0.25756519]]\n",
            "w^(4) \n",
            "[[0.89074905 0.87952098 0.28830346 0.00090785]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:792: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, order=order, subok=subok, copy=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cost derivates\n",
            "D^(2) \n",
            "[[ 0.00202524 -0.00108029 -0.00108029]\n",
            " [ 0.00051762 -0.00027145 -0.00027145]\n",
            " [ 0.000509   -0.00026024 -0.00026024]]\n",
            "D^(3) \n",
            "[[6.22111314e-03 3.29671904e-03 3.15167092e-03 3.41357878e-03]\n",
            " [2.37477996e-03 1.25666950e-03 1.20117566e-03 1.30241241e-03]\n",
            " [6.65390939e-06 3.52533398e-06 3.37015459e-06 3.65096478e-06]]\n",
            "D^(4) \n",
            "[[0.04399095 0.03513973 0.03302529 0.03470487]]\n",
            "\n",
            "Approximate derivates (numerical method)\n",
            "D_approx^(2) \n",
            "[[Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]\n",
            " [Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]\n",
            " [Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]]\n",
            "D_approx^(3) \n",
            "[[Decimal('0E-59') Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]\n",
            " [Decimal('0E-59') Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]\n",
            " [Decimal('0E-59') Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]]\n",
            "D_approx^(4) \n",
            "[[Decimal('0E-59') Decimal('0E-59') Decimal('0E-59') Decimal('0E-59')]]\n",
            "\n",
            "Final weights after training\n",
            "w^(2) \n",
            "[[0.6042215  0.72422298 0.14450085]\n",
            " [0.57732773 0.05078764 0.93801944]\n",
            " [0.93374964 0.46618774 0.8854764 ]]\n",
            "w^(3) \n",
            "[[0.56491708 0.97390104 0.22779824 0.31540141]\n",
            " [0.50721156 0.83774705 0.25711768 0.03453969]\n",
            " [0.58088873 0.50763254 0.64999732 0.25756154]]\n",
            "w^(4) \n",
            "[[ 0.8467581   0.84438125  0.25527817 -0.03379702]]\n",
            "\n",
            "Costs for all epochs\n",
            "[0.1895693003780664, 0.18389108504200885]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXj37Cfr31er"
      },
      "source": [
        "### Train network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MybmtkiwLASq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "c00445d6-c082-46ec-9f34-99b2000b7db2"
      },
      "source": [
        "# Initialize weights and plot initial decision frontier\n",
        "# s_l always needs 2 at beginning and 1 at the end to comply with data format\n",
        "# s_l = [2, 1]\n",
        "s_l = [2, 3, 3, 1]\n",
        "initial_weights = initializeWeights(s_l)\n",
        "w = initial_weights.copy()\n",
        "\n",
        "# Plotando fronteira de decisão\n",
        "x1s = np.linspace(-1,1.5,50)\n",
        "x2s = np.linspace(-1,1.5,50)\n",
        "zs=np.zeros((len(x1s),len(x2s)))\n",
        "\n",
        "for i in range(len(x1s)):\n",
        "    for j in range(len(x2s)):\n",
        "        xs = np.array([[x1s[i]],[x2s[j]]])\n",
        "        zss, yss = fowardPropagate(xs, w, s_l)\n",
        "        zs[i,j] = zss[-1][0,0] # saida do modelo antes de aplicar a função sigmoide \n",
        "plt.contour(x1s,x2s,np.transpose(zs),0)\n",
        "\n",
        "df=pd.read_csv(\"test_data.txt\", header=None)\n",
        "\n",
        "ins=df.iloc[:,:-1].values\n",
        "outs=df.iloc[:,-1].values\n",
        "pos , neg = (outs==1).reshape(100,1) , (outs==0).reshape(100,1)\n",
        "plt.scatter(ins[pos[:,0],0],ins[pos[:,0],1],c=\"r\",marker=\"+\")\n",
        "plt.scatter(ins[neg[:,0],0],ins[neg[:,0],1],marker=\"o\",s=10)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.legend([\"Accepted\",\"Rejected\"],loc=0)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe4232d8e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXjU5b338feXEAggiwIqsgYEZEvCliGIgrUsthXLUYvYY7EuaD0Y2nPVq/bxOdWny6mn7bmOoohaQUVpoGJF9HiOqBSXAiEBwi4CKjUUAVkTyM79/JFJGGI2kpn5zfJ5XVeuZGZ+mbnzS+Tj977v+f7MOYeIiEhTtfB6ACIiEt0UJCIi0iwKEhERaRYFiYiINIuCREREmkVBIiIizeJpkJjZQjM7ZGbb6nh8gpmdMLM8/8cvwj1GERGpX0uPX/8F4ElgUT3HfOic+054hiMiIufL04rEOfcBcNTLMYiISPN4XZE0RoaZbQb+AfzUObe95gFmNguYBdCuXbuRV1xxRZiHKCISZXbtgqIiaNMGCgv5HPjKOWvKU5nXLVLMrA/wpnNuaC2PdQDOOOcKzexbwOPOuf71Pd+oUaNcbm5uSMYqIhIzJkyo/Lx6NUyYwKD33y/c6Vz7pjxVRFckzrmTAV+/ZWZPmVkX59xXXo5LRCTqVAVHlfffP3t/Xl6znjqit/+a2aVmZv6v06kc7xFvRyUiEmPS0vgYdjX12z2tSMwsC5gAdDGzfOBhIBHAOfc0cBPwIzMrB4qAW5zXc3EiItGiqtpISztbgYwff+7n1asrP1uTlkcAj4PEOTejgcefpHJ7cLOUlZWRn59PcXFxc58qriUlJdGjRw8SExO9HoqIRJCIXiMJlvz8fNq3b0+fPn2wZqRuPHPOceTIEfLz80lOTvZ6OCJSl9qqEICOHSvvq6pAgiii10iCpbi4mM6dOytEmsHM6Ny5s6o6EfmauKhIAIVIEOgcikSgunZjQUirkEBxUZGIiEjoKEjCbPny5ZgZH3/8cVhe79///d/P+3teeOEFZs+eHYLRiEjQTJhQ+fH+++dWIePHV36sXg3Hj4e8GgEFSdhlZWUxbtw4srKywvJ6TQkSEYlwQXgTYTApSOpSlfZBVFhYyEcffcSCBQtYsmQJABUVFfz0pz9l6NChpKSk8MQTTwCQk5PD2LFjSU1NJT09nYKCAioqKnjggQcYPXo0KSkpPPPMMwCsXr2aq6++mm9/+9sMHDiQe++9lzNnzvDggw9SVFREWloa3//+9wF4+eWXSU9PJy0tjXvuuYeKigoAnn/+eQYMGEB6ejp/+9vfgvpzi0gIVK19BFYggR9hFDeL7ZHg9ddfZ8qUKQwYMIDOnTuzYcMG1q9fz+eff05eXh4tW7bk6NGjlJaWMn36dJYuXcro0aM5efIkbdq0YcGCBXTs2JGcnBxKSkq48sormTRpEgDr169nx44d9O7dmylTpvCXv/yFRx99lCeffJI8//+57Ny5k6VLl/K3v/2NxMRE7rvvPhYvXszEiRN5+OGH2bBhAx07duSaa65h+PDhXp4qEamptm29gfd5SEFSU1UVEviLgqAkfFZWFnPmzAHglltuISsri88++4x7772Xli0rfxUXXXQRW7dupVu3bowePRqADh06ALBy5Uq2bNnCsmXLADhx4gS7d++mVatWpKen07dvXwBmzJjBRx99xE033XTO67/33nts2LCh+nmLioq4+OKLyc7OZsKECXTt2hWA6dOn88knnzT75xWRMAjDrqyGKEjC5OjRo6xatYqtW7diZlRUVGBm1f+oN4ZzjieeeILJkyefc//q1au/tjW3tq26zjlmzpzJb3/723PuX758+Xn8JCISFhGwrbextEZSU9X8Ys15x2ZatmwZt912G/v27ePzzz/niy++IDk5mdTUVJ555hnKy8uBysAZOHAgBw4cICcnB4CCggLKy8uZPHky8+fPp6ysDIBPPvmEU6dOAZVTW5999hlnzpxh6dKljBs3DoDExMTq46+99lqWLVvGoUOHql9r3759+Hw+3n//fY4cOUJZWRmvvPJKs39eEYkfqkjCJCsri5/97Gfn3HfjjTeyc+dOevXqRUpKComJidx9993Mnj2bpUuXcv/991NUVESbNm149913ueuuu/j8888ZMWIEzjm6du1aXU2MHj2a2bNns2fPHq655hqmTZsGwKxZs0hJSWHEiBEsXryYX//610yaNIkzZ86QmJjIvHnzGDNmDI888ggZGRl06tSJNI/nW0Xi1vk0WYwgnl/YKthqu7DVzp07GTRokEcjCr3Vq1fzhz/8gTfffDPkrxXr51LEU/UFSZUQBYmZbXDOjWrK96oiERHxShStg9RHQRIDJkyYwIQgv+dFRKSxFCQiIuFW820GUbAOUh/t2hIRCacIa28SDKpIRETCrWrtI4hvePaSKhIRkVCbMAE6dTq3W28MVSYKkjBJSEggLS2NoUOHcv3113P8+PF6j3/66adZtGjReb/O8ePHeeqpp877+x555BH+8Ic/nPf3iUgzRMmurIZoaitM2rRpU908cebMmcybN4+HHnqozuPvvffeJr1OVZDcd999Tfp+EQmCGNnW21iqSDyQkZHB/v37Adi7dy9Tpkxh5MiRXHXVVdUXvAqsEOo65uDBg0ybNo3U1FRSU1NZs2YNDz74IHv37iUtLY0HHngAgN///vfVrecffvjh6nH85je/YcCAAYwbN45du3aF8xSISAxRRVKHd3Yc5MPdh7mqf1cmDr4kaM9bUVHBe++9x5133glUtjB5+umn6d+/P9nZ2dx3332sWrXqnO+p65jMzEzGjx/Pa6+9RkVFBYWFhTz66KNs27atuvpZuXIlu3fvZv369TjnmDp1Kh988AHt2rVjyZIl5OXlUV5ezogRIxg5cmTQfk6RuBOl7U2CQUFSi3d2HCQzaxNFZRW8kpvP3BnDmx0mVReY2r9/P4MGDWLixIkUFhayZs0abr755urjSkpKzvm++o5ZtWpV9TpKQkICHTt25NixY+d8/8qVK1m5cmX19UUKCwvZvXs3BQUFTJs2jbZt2wIwderUZv18IhK/FCS1+HD3YYrKKq8cWFRWwYe7Dzc7SKrWSE6fPs3kyZOZN28et99+O506daquHmpz5syZBo+pj3OOn//859xzzz3n3P/YY4816flEJEBtVQjE5DpIfbRGUour+nelTWICAG0SE7iqf9egPXfbtm2ZO3cu//mf/0nbtm1JTk6ubtvunGPz5s3nHN+hQ4c6j7n22muZP38+UDllduLECdq3b09BQUH190+ePJmFCxdSWFgIwP79+zl06BBXX301y5cvp6ioiIKCAt54442g/YwiEl8UJLWYOPgS5s4Yzg8yegdlWqum4cOHk5KSQlZWFosXL2bBggWkpqYyZMgQXn/99erjqi5OVdcxjz/+OH/9618ZNmwYI0eOZMeOHXTu3Jkrr7ySoUOH8sADDzBp0iRuvfVWMjIyGDZsGDfddBMFBQWMGDGC6dOnk5qaynXXXXdeF9gSiXsTJpx9T8iJE5X3dex49hpGx4/HTTUCaiMfse6//35GjBjBD3/4Q6+Hco5oPJciQVPXtt7x489OcUVpgKiNfIz5t3/7N7Kzs3nkkUe8HoqI1KYqLGKkxUlzKUgi0K9+9St+9atfeT0MEYHaF9R12YZzxE2QOOeq1xykaWJtGlSk2eK8EqkSF0GSlJTEkSNH6Ny5s8KkiZxzHDlyhKSkJK+HIhJacdbeJBjiIkh69OhBfn4+hw8f9nooUS0pKYkePXp4PQwRiTBxESSJiYkkJyd7PQwRiVRx3N4kGPQ+EhERaRZPKxIzWwh8BzjknBtay+MGPA58CzgN3O6c2xjeUYpITFJ7k6DxuiJ5AZhSz+PXAf39H7OA+WEYk4iInAdPKxLn3Adm1qeeQ24AFrnKfafrzKyTmXVzzh0IywBFJHZoN1bIeF2RNKQ78EXA7Xz/fecws1lmlmtmudqZJSISXjGxa8s59yzwLFT22vJ4OCISSaoqEe3GCplIr0j2Az0Dbvfw3yci0rCqBXUJqUivSFYAs81sCeADTmh9RETOS9XahxoshozX23+zgAlAFzPLBx4GEgGcc08Db1G59XcPldt/I6unuohElroW1AO3+krQeb1ra0YDjzvgX8I0HBGJZdqVFTKRPrUlIlI/tTfxXKQvtouISIRTRSIi0UftTSKKKhIREWkWVSQiEvnU3iSiqSIREZFmUUUiIpFL7U2igioSEYlMam8SNVSRiEjkUnuTqKCKREQix4QJ0KlT5ef336/8UGUS8RQkIhL5tCsromlqS0S8o229MUEViYiINIsqEhEJLzVZjDmqSEREpFlUkYhIaGkdJOapIhERkWZRRSIioaH2JnFDFYmIBJ/eRBhXVJGISGiovUncUEUiIsGh9iZxS0EiIqGlXVkxT1NbItI02tYrfqpIRESkWVSRiEjjqb2J1EIViYiINIsqEhGpX21VCGgdRKqpIhERkWZRRSIitavZ4gRUhUitFCQicq6a23pFGqAgEZHaVVUdanEiDVCQiEjtC+qqTKSRFCQiUj9VItIABYlIPFJ7EwkiT7f/mtkUM9tlZnvM7MFaHr/dzA6bWZ7/4y4vxikiInXzrCIxswRgHjARyAdyzGyFc25HjUOXOudmh32AIrFG7U0kRLysSNKBPc65T51zpcAS4AYPxyMiIk3g5RpJd+CLgNv5gK+W4240s6uBT4CfOOe+qHmAmc0CZgH06tUrBEMViVJqbyJhEOktUt4A+jjnUoB3gBdrO8g596xzbpRzblTXrl3DOkARkXjnZUWyH+gZcLuH/75qzrkjATefA34XhnGJRD+1N5Ew8jJIcoD+ZpZMZYDcAtwaeICZdXPOHfDfnArsDO8QRaKM3kQoHvAsSJxz5WY2G3gbSAAWOue2m9kvgVzn3Aog08ymAuXAUeB2r8YrElXU3kTCyNM3JDrn3gLeqnHfLwK+/jnw83CPSySqqL2JeEzvbBeJZapEJAwUJCLRRu1NJMJE+vZfERGJcKpIRKKB2ptIBFNFIiIizaKKRCRSqb2JRAlVJCIi0iyqSEQijdqbSJRRkIhECr2JUKKUgkQk0qi9iUQZBYmIl9TeRGKAgkQkUqkSkSihIBEJN23rlRhT7/ZfM+tgZv1quT8ldEMSEZFoUmdFYmbfAx4DDplZInC7cy7H//ALwIjQD+/8HTxVyD+/9go/GTOWkd26ez0ckbO0rVdiVH0Vyf8BRjrn0oAfAi+Z2TT/YxbykTVRYosEdn31FTe/soQfvv4qm7880PA3iYTQOzsO8ovXt/HOhV8r7kVigjnnan/AbKtzbljA7W7Am8CLVFYnEVmRjBo1yn2wdi0vb8njmQ3rOVZczDf69OUnY8Yy5OJLvB6exJl3dhwkM2sTRWUVtElMYO6M4Uy8b3rlg6pCJIKY2Qbn3KimfG99i+0FZtbPObcXwDl3wMwmAMuBIU15sXBpm5jIrJGjuXVYKi9u3sRzG3O5fsnLTOp7OXN8GQzqerHXQ5QY986Og3y4+zBfHD1NUVkFAEVlFXy4+zATPR6bSLDVV5GkAqeBROfcjoD7E4FbnHMvhWeI52fUqFEuNzf3nPtOlpTwfN4GFm7aSEFpCdddPoBMXwYDO3fxaJQSywKrkFYJlbPHpRVnzlYkg1UZS+QJSUXinNvsf/JtZvYS8Dsgyf95FBCRQVKbDq1bM8c3lttTR7AwbwPPb9rI/+75hG8PGMic9Az6XdTZ6yFKDPlw9+HqKqS04gzXDOxKz4vaclX/rgoRiUl1ViTVB5i1A/4DGAm0BxYD/+GcOxP64Z2/2iqSmo4VFfHcplxe3LyJ4vJypg64gvt9GSR3ujBMo5RYUzWVdVX/rgBfXxdRgEiEa05F0pggaQX8BpgIXAD8X+fckqa8WDg0JkiqHDl9mj9uyuWlzZsorajgu1cM5v70MfTq2CnEo5RYUtuCOlAdLAoRiQahDpLNwOvAr4AuwNNAqXPu5qa8YKidT5BUOXz6FM9uyOHlLZspP1PBjYOGMDt9DD06dAzRKCUWBC6o/3XX4er7f5DRm1/eMNTDkYmcv1Dt2qpyp3Ou6l/mA8ANZnZbU14sUnVt246HrprA3SNGMT93PVnbtvCXj3dw8+Ch3DfaR/f2HbweokSYmgvqrRJaVC+oV01vicSLBiuSaNOUiqSmLwsLmJ+7nqXbtuJwTB8yjPtG+7j0gvZBGqVEq7qqEC2oS7QL6dRWtAlGkFTZX3CSp3KyWbZjG2bGrUNTuHdUOhe3uyAozy/RRdt6JZYpSAIEM0iq5J88wZPr1/Hqzu20bJHA94elcs+o0XRt2y6oryORJ3A31oe7D7No7b7qx1SFSCxRkAQIRZBU2Xf8OE+sX8vyXTtplZDAD1LSuHvEaDq3bRuS1xNv1dyNdce4ZBZ+9Jm29UpMUpAECGWQVPn02FGeXL+OFZ98TFLLlsxMHc5dw0dxYZs2IX1dCY/6dmNVVSaqQiTWKEgChCNIquw9eoS569fx5icf0y6xFbenjeDO4SPpmJQUlteX4NM6iMSrUG//lTr0u6gzj0/5Nv8y2sfc7LU8mbOOFzZv5I60kdwxfAQdWitQoo3am4icP1UkQbTzq8M8nr2GlXv30KF1a+4cPpLbU0fQvnVrT8YjjaP2JiKa2jqHl0FSZduhgzyevYb3PvuUTklJ3Dl8FDNTh3NBq1aejku+Tu1NRCopSAJEQpBU2XLwSx7PXstfP/+UC5OSuHvEaG5LSaOdAsVzam8ici6tkUSolEsuZcHUaeR9eYDHs9fwuzUfsmBTLrNGjuafh6XRJjHR6yHGJbU3EQkuTysSM5sCPA4kAM855x6t8XhrYBGVLeyPANOdc5/X95yRVJHUtPHAP3hs3Ro++mIfXdq25Z6R6Xx/WApJLRUo4aD2JiJ1i8qpLTNLAD6hsj19PpADzKhxNcb7gBTn3L1mdgswzTk3vb7njeQgqZLzj3weW7eWtfl/p2vbdvxoVDozhqbQuqUKxFDRtl6R+kVrkGQAjzjnJvtv/xzAOffbgGPe9h+z1sxaAl8CXV09g46GIKmSnf8Fj2WvIXt/Ppe0u4AfjUpn+pBhCpQgUXsTkcaL1jWS7sAXAbfzAV9dxzjnys3sBNAZ+CrwIDObBcwC6NWrV6jGG3S+Hj3J6jGdtV/8ncey1/DI+6t4Onc99432cfPgoQqUZgisQF7JzeeOccm0SUyo3p11q6+3AkQkSGLiXyrn3LPAs1BZkXg8nPOW0bMXY3r0ZE3+33ls3Rp+sfo95ueuZ3b6GG4cNIRWCQleDzFqBK6DVL2xsKisgoLiMubOGK5tvSIh4GWQ7Ad6Btzu4b+vtmPy/VNbHalcdI85ZsaVPXsztkcvPvr7Pv5r3RoeWvUOT+VkM3u0j38aNIREBUq9GtqNNXHwJQoQkRDwMkhygP5mlkxlYNwC3FrjmBXATGAtcBOwqr71kVhgZlzVuw/jevXm/X2f81j2Gn6+6h2e8lco064YTMsWLbweZkRSexMRb3gWJP41j9nA21Ru/13onNtuZr8Ecp1zK4AFwEtmtgc4SmXYxAUzY0KfZMb37sOqzz/l8ey1/Ozdt3kqJ5v708cwdeAgBYpf1XRW+6RErYOIeEDvbI8Szjne+2wvj61bw46vDtOn04Vkpo/h+gFXkBDHgVLbNUMKistUhYicp2jdtSXnwcz4Zt/LuTa5Hys/3cPj2Wv515X/w5M567g/PYPv9B8YN4FSc1tvzUV1tTgRCa/4+JcnhpgZk/v1580ZtzHvW9fTskUCP3n7La5b/CJvfvIxZ2KswqypqgJZtHYfmVmbqqezALU4EfGIprai3Bnn+J/dnzB3/Vp2Hz3CgIs6k+nLYMrlA2hh5vXwgkZXLRQJrah8Z3uoxFuQVKk4c4a39nzC3Oy17D12lIGduzDHN5ZJ/S6P+kBRexOR0NMaiZDQogXXD7iCb10+gDd372Ju9lrue2sFg7p0ZY4vg4l9L8eiKFDqWgfRtl6RyKMgiTEJLVpww8BBfLv/QN7Y9TFz16/l3v9ewZCuFzPHl8G1yf0iPlDU3kQkuihIYlTLFi2YNmgw1w+8ghW7dvLE+nXMevN1hna9mDm+sXwjuW/EBYram4hEJ62RxInyM2dY/vEOnly/jr+fPMGwiy9hjm8s1/RJjohA0TqIiLe0RiINatmiBTcNHsoNAwfx2sc7mJeTzV1vvEbqJZcyxzeW8b37eBooWgcRiV4KkjiTmJDA94YMY9oVg/nLzu08mZPNHSv+Qtol3Zjjy+DqMAeK2puIRD9NbcW50ooKXt25nXk56/hHQQHDL+3Gj31jGderd8gDRe1NRCKHprakyVolJDBjaAo3DhrCsh3bmJeTzczXX2Vkt8vI9GUwrmdwA0XtTURij4JEgMpAuXVYamWg7NzOUznrmLn8VUZd1p05vgzG9ujV7EBpaFuv2puIRCdNbUmtSsrL+fOObczPyebLU4WMvqw7P/aNJaPn+V/KWO1NRCKfWqQEUJAEV0l5OUu3b2V+7noOnirE170Hc3xjGdOjZ8PfjLb1ikQLBUkABUlolJSXs2T7FubnrufQqVOM6d6TH48ZS3r3HrUeX1cVom29IpFJQRJAQRJaxeVlLNlWWaEcPn2KjB69mOPLOCdQVIWIRB8FSQAFSXgUl5fxp61beHrDer46fZorLrqUXom9uHHYAD7cfZhFa/dVH6sqRCTyafuvhF1Sy0TuGD6SGUNTePjdj1i2azMft/iSd77YzncvT9WbC0XiiIJEmqxqHeTw0XacOdgP2h7DLjjKa39fw+CB3ejZqhf/NLS/QkQkxilIpElqroO0atGS0lOdaV3ahe9kJLE6/2N2HD1AUauDdLkwg+HdLvN6yCISIgoSaZKGmiyeKh3Hy1vz+OOGXG58JYvxvfswxzeWtEu7eTxyEQk2LbZLowW2NwHO6ZNV126sU6Wl1YFytLiI8b2T+bEvg1QFikhE0a6tAAqS0KjZYHHujOEAjX5X+qnSUl7akscfN+ZwrLiYCX2SmZOuQBGJFAqSAAqS4KqvvUlTGiwWlpby0pZN/HFjLscVKCIRQ9t/JSS+tqCe0KL6jYVNbbB4QatW/GiUj9tShrNo8yae25TLtD//iWv69GWOL4OUSy4N8k8hIqGmikS+JpztTQpKSli0JY8FmyorlG/06UumAkUk7DS1FUBB0jxetTepCpTnNuZyoqSYa5P7Msc3lqEX6z0oIuGgIAmgIDl/NS825WV7k8pA2cRzGzdwoqSYbyb3I9OXoUARCTEFSQAFyfmp7XK3Cz/6rMFtvaF2sqSkeg3lZEkJE/v2IzM9gyEKFJGQUJAEUJA0TrRcbOpkSQkvbt7Igk0bOFlSwqS+l5Ppy2Bw14s9HZdIrFGQBFCQNCwa27yfLCnhhbzKQCkoVaCIBJu2/8p5aai9SSTq0Lo1mb4Mbk8bzvN5G1m4aSMrP93DpH6Xk5muQBHxkiqSONGU9iaR7GRJcXWgFJSWMKnf5cxJz2CQAkWkSaJuasvMLgKWAn2Az4HvOeeO1XJcBbDVf/PvzrmpDT23guTrmtveJJKdLClm4aaNLMzbQGFpKZP79SczfYwCReQ8RWOQ/A446px71MweBC50zv2sluMKnXMXnM9zK0jOCnZ7k0h2othfoQQGii+DQV2a9g58kXgTjWskNwAT/F+/CKwGvhYk0nShaG8SyTomJfHjMWP5YdoIFuZt4Pm8jby9dzdT+vXnfgWKSEh5VZEcd8518n9twLGq2zWOKwfygHLgUefc8jqebxYwC6BXr14j9+3bV9thcSGc7U0iWc0KRYEiUr+InNoys3eB2homPQS8GBgcZnbMOXdhLc/R3Tm338z6AquAa51ze+t73Xie2orGbb2hpkARaZyInNpyzn2zrsfM7KCZdXPOHTCzbsChOp5jv//zp2a2GhgO1Bsk8aZme5No29YbaoFTXlWB8r+a8hIJKq/WSFYAM4FH/Z9fr3mAmV0InHbOlZhZF+BK4HdhHWWEC6xAXsnN545xybRJTKjenXWrr3fcBkhNdQWKdnmJNJ9XaySdgT8DvYB9VG7/PWpmo4B7nXN3mdlY4BngDNACeMw5t6Ch546Hqa1oaW8SyU4UF1cvymvbsEiErpF4JdaDROsgwVVroGjKS+JQRK6RSGhoHSS4OiYl8ZMxV3JH2khtGxZpIgVJlKiazmqflKh1kBCoLVC0KC/SOJraigK1XTOkoLhMVUgI6Z3yEm80tRWD6trWW1RWQUFxWcy1OIk0te3y0pSXSO0UJBGooW29sdjiJFLV1npFU14i59LUVgTRtt7IV3OXlwJFYoW2/waI1iDRtt7oovehSKzRGkmUUnuT6FXXtmEFisQjVSQeqW0n1sKPPovqqxbGs5oVii4BLNFGFUkUCVwHqbkTa+6M4VoHiVKBFcrzeRt5Pm8jK/fuYVLfy8n0KVAktqkiCSOtg8SPr11Tvu/l3J8+hiEX63cskUkVSZTQOkj86NA6iTm+ym3DL+RtYsGmDaz8dA8T+/YjMz1DgSIxRUESBmpvEr86tE4i05fB7WnDeSFvEwvzNnD9kpf5ZnI/Mn0ZDFWgSAzQ1FaIqb2JBDpZUsKLmzeyYNMGTpaUcG1yXzJ9YxmmQBGPaWorwqi9idSlQ+vW3J+ewczUESzaXDnldcOSl/lGn75k+jJIuaS2q1OLRLYWXg8g1lRVIIvW7iMza1P1dBag9iZSrUPr1sxOH8P7t9/Fv465kg0H/sF3ly7mrhWvseXgl14PT+S8aGorSNTeRJqjoKSEF/0VyomSYq7xVyipqlAkTNQiJYAXQaJtvRIsBSUlLNqSx4JNuRwvVqBI+ChIAoQzSOqqQrStV5qrsLSURZs38Zw/UCb0SWZOegapl3bzemgSoxQkAcIVJKpCJBwUKBIuCpIAoQySmruxFq3dV/2YqhAJpcLSUl7asok/bqwMlPG9k5njyyBNgSJBoiAJEKogUZNFiQRVgfLcxlyOFRdzda8+ZPoyGNHtMq+HJlFO7yMJg9reD6ImixJuF7RqxY9G+fhBynBe2rGkwlYAAAf1SURBVJLHcxtzuemVLK7q1ZtMXwYju3X3eogSh1SRNCCwvYkqEIk0p0pLWbx1M89uyOFocRHjelYGyqjLFChyfjS1FSCYQaL2JhItTpeVsXhrHs9uyOFIURFX9uxFpi+D0Zf18HpoEiU0tRVEam8i0ahtYiJ3jxjN94el8aetm3lmQw7Tly0lo0cv5vgySO+uQJHQUYuUAGpvItGubWIid40YxQe338X/GTee3Ue/4pZXl3Lrq38mO/8Lr4cnMUpTW6i9icSuorIysrZt4ZkNORw+fQpf9x7M8Y1lTI+eXg9NIozWSAKcb5DojYUSD4rLy8jatpVnNqzn0KlTpF/Wgzm+DMb06ImZeT08iQAKkgCNDRK1N5F4VFxexpJtW3naHyijL+vOHN9YMhQocU9BEqAxQaIqROJdSXk5S7dv5enc9Xx5qpBRl3Vnji+DsT16KVDilHZtNUJdu7F07XSJR61btuQHqcOZPmRYdaDc9toyRna7jExfBuN69lagSKPFRUWi9iYi9SspL+fPO7bxdG42BwoLGXFpN+b4xjKulwIlXqgiaYDam4jUr3XLltyWksb3Bg/llR3bmJ+7npmvv8rwS7uRmZ7B1b37KFCkTjEdJIHtTdokJlRXIFXhoQAROVfrli3555Q0bh48lFd3buep3Gx+uOIvpF3SjUxfBuMVKFILT6a2zOxm4BFgEJDunKt1ddzMpgCPAwnAc865Rxt67qqpLbU3EWm+0oqKykDJyWZ/wUlSLrmUzPQMrumTrECJMdE4tbUN+CfgmboOMLMEYB4wEcgHcsxshXNuR31P/I/jRdWViNqbiDRPq4QEZgxN4cZBQ3ht53bm5WZz1xuvMeziS8j0ZfCNPn0VKOJNixTn3E7n3K4GDksH9jjnPnXOlQJLgBsaeu4jp0rV3kQkyFolJDB9aArv3XYHv712EseLi7n7jeV8euyo10OTCODpri0zWw38tLapLTO7CZjinLvLf/s2wOecm13LsbOAWQAktBzZqmsfKopOHnIlp09a67YdXMnpk2eKC0+E8meJUF2Ar7weRITQuThL5+IsnYuzBjrn2jflG0M2tWVm7wKX1vLQQ86514P5Ws65Z4Fn/a+bW3Jgd5Pm+WKNmeU2dc4z1uhcnKVzcZbOxVlm1uTrb4QsSJxz32zmU+wHAjvL9fDfJyIiESSS28jnAP3NLNnMWgG3ACs8HpOIiNTgSZCY2TQzywcygP82s7f9919mZm8BOOfKgdnA28BO4M/Oue2NePpnQzTsaKRzcZbOxVk6F2fpXJzV5HMRcy1SREQkvCJ5aktERKKAgkRERJol6oPEzG42s+1mdsbM6tzGZ2ZTzGyXme0xswfDOcZwMbOLzOwdM9vt/3xhHcdVmFme/yOmNjA09Hs2s9ZmttT/eLaZ9Qn/KMOjEefidjM7HPC3cJcX4ww1M1toZofMbFsdj5uZzfWfpy1mNiLcYwyXRpyLCWZ2IuBv4heNed6oDxLOtlv5oK4DAtqtXAcMBmaY2eDwDC+sHgTec871B97z365NkXMuzf8xNXzDC61G/p7vBI455y4H/gv4j/COMjzO429+acDfwnNhHWT4vABMqefx64D+/o9ZwPwwjMkrL1D/uQD4MOBv4peNedKoD5JQtluJQjcAL/q/fhH4rodj8UJjfs+B52gZcK3FZrOoePmbb5Bz7gOgvl4uNwCLXKV1QCcz6xae0YVXI85Fk0R9kDRSd+CLgNv5/vtizSXOuQP+r78E6mpznGRmuWa2zsxiKWwa83uuPsa/xfwE0Dksowuvxv7N3+ifzllmZj1reTwexMu/D42VYWabzex/zGxIY74hKq5HEs52K5GuvnMReMM558ysrr3dvZ1z+82sL7DKzLY65/YGe6wS8d4AspxzJWZ2D5WV2jc8HpN4ayOV/z4Umtm3gOVUTvnVKyqCRO1WzqrvXJjZQTPr5pw74C/ND9XxHPv9nz/1N84cDsRCkDTm91x1TL6ZtQQ6AkfCM7ywavBcOOcCf+7ngN+FYVyRKGb+fWgu59zJgK/fMrOnzKyLc67expbxMrUVL+1WVgAz/V/PBL5WrZnZhWbW2v91F+BKoN5rvESRxvyeA8/RTcAqF5vvym3wXNRYB5hKZQeJeLQC+IF/99YY4ETAFHFcMbNLq9YMzSydyoxo+H+0nHNR/QFMo3JOswQ4CLztv/8y4K2A474FfELl/3k/5PW4Q3QuOlO5W2s38C5wkf/+UVReYRJgLLAV2Oz/fKfX4w7yOfja7xn4JTDV/3US8AqwB1gP9PV6zB6ei98C2/1/C38FrvB6zCE6D1nAAaDM/2/FncC9wL3+x43KHW57/f9NjPJ6zB6ei9kBfxPrgLGNeV61SBERkWaJl6ktEREJEQWJiIg0i4JERESaRUEiIiLNoiAREZFmUZCIhJGZ/a+ZHTezN70ei0iwKEhEwuv3wG1eD0IkmBQkIiFgZqP9zRCTzKyd/5o5Q51z7wEFXo9PJJiioteWSLRxzuX4Lxr2a6AN8LJzrtaLCYlEOwWJSOj8ksqeV8VApsdjEQkZTW2JhE5n4AKgPZU9vkRikoJEJHSeAf4NWEyMXtJXBDS1JRISZvYDoMw59yf/9dPXmNk3gP8HXAFcYGb5VHZfftvLsYo0l7r/iohIs2hqS0REmkVBIiIizaIgERGRZlGQiIhIsyhIRESkWRQkIiLSLAoSERFplv8P4K0bPnWCAPwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yWA2US_v_QEK",
        "outputId": "3271164b-5ff9-4bfa-f7e6-1d03c80d3c9d"
      },
      "source": [
        "num_epochs = 1000\n",
        "lambdaRate = 0.0\n",
        "alphaRate = 1.0\n",
        "\n",
        "costsList = []\n",
        "num_training = 90 # Maximum is 118\n",
        "num_validation = len(training_data) - num_training\n",
        "\n",
        "print(\"Network shape: s_l = \", end='')\n",
        "print(s_l)\n",
        "print(\"Training sets used: \" + str(num_training))\n",
        "print(\"Alpha rate: \" + str(alphaRate) + \" Lambda rate: \" + str(lambdaRate))\n",
        "print(\"Training epochs: \" + str(num_epochs), end=\"\\n\\n\")\n",
        "\n",
        "# Actual training starts here\n",
        "print(\"Initial weights\")\n",
        "for s in range(len(w)):\n",
        "    print(\"w^(%s) \" % (s+2))\n",
        "    print(w[s])\n",
        "\n",
        "# Train num_epoch times using the training set\n",
        "for epoch in range(num_epochs):\n",
        "    Delta = initializeDelta(s_l)\n",
        "    cost = 0\n",
        "    for m in range(int(num_validation/2), int(num_training+(num_validation/2))):\n",
        "        x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "        t = np.array([[float(training_data[m][2])]])\n",
        "        z, y = fowardPropagate(x, w, s_l)\n",
        "        cost += calculateSumError(y[-1], t, s_l)\n",
        "        delta = backPropagateError(t, y, w, s_l) \n",
        "        Delta = updateDelta(x, Delta, y, delta, s_l)\n",
        "    cost = cost/num_training\n",
        "    D = calculateWeightAdjust(Delta, w, s_l, num_training, lambdaRate)\n",
        "    for s in range(len(w)):\n",
        "        w[s] = np.copy(w[s] - alphaRate*D[s])\n",
        "    costsList.append(cost)\n",
        "\n",
        "# Caculate the cost one more time for final epoch\n",
        "cost = 0\n",
        "for m in range(int(num_validation/2), int(num_training+(num_validation/2))):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    cost += calculateSumError(y[-1], t, s_l)\n",
        "cost = cost/num_training\n",
        "costsList.append(cost)\n",
        "\n",
        "print(\"\\nFinal weights after training\")\n",
        "for s in range(len(w)):\n",
        "    print(\"w^(%s) \" % (s+2))\n",
        "    print(w[s])\n",
        "\n",
        "print(\"\\nCosts for all training epochs\")\n",
        "print(costsList)\n",
        "\n",
        "# Validation\n",
        "print(\"Validation test\")\n",
        "hits = 0.0\n",
        "for m in range(int(num_validation/2)):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    if (abs(t[0,0]-y[1][0,0]) <= 0.5):\n",
        "        hits += 1.0\n",
        "    print(\"Training data[\" + str(m) + \"]: Network output: \" + str(y[-1][0,0]) + \" Expected output: \" + str(t[0,0]))\n",
        "for m in range(int(num_training+(num_validation/2)), int(len(training_data))):\n",
        "    x = np.array([[float(training_data[m][0])],[float(training_data[m][1])]])\n",
        "    t = np.array([[float(training_data[m][2])]])\n",
        "    z, y = fowardPropagate(x, w, s_l)\n",
        "    if (abs(t[0,0]-y[1][0,0]) <= 0.5):\n",
        "        hits += 1.0\n",
        "    print(\"Training data[\" + str(m) + \"]: Network output: \" + str(y[-1][0,0]) + \" Expected output: \" + str(t[0,0]))\n",
        "accuracy = hits / num_validation\n",
        "print(\"Model accuracy: \" + str(accuracy))\n",
        "\n",
        "# Plotando fronteira de decisão\n",
        "x1s = np.linspace(-1,1.5,200)\n",
        "x2s = np.linspace(-1,1.5,200)\n",
        "zs=np.zeros((len(x1s),len(x2s)))\n",
        "\n",
        "for i in range(len(x1s)):\n",
        "    for j in range(len(x2s)):\n",
        "        xs = np.array([[x1s[i]],[x2s[j]]])\n",
        "        zss, yss = fowardPropagate(xs, w, s_l)\n",
        "        zs[i,j] = zss[-1][0,0] # saida do modelo antes de aplicar a função sigmoide \n",
        "plt.contour(x1s,x2s,np.transpose(zs),0)\n",
        "\n",
        "df=pd.read_csv(\"test_data.txt\", header=None)\n",
        "\n",
        "ins=df.iloc[:,:-1].values\n",
        "outs=df.iloc[:,-1].values\n",
        "pos , neg = (outs==1).reshape(100,1) , (outs==0).reshape(100,1)\n",
        "plt.scatter(ins[pos[:,0],0],ins[pos[:,0],1],c=\"r\",marker=\"+\")\n",
        "plt.scatter(ins[neg[:,0],0],ins[neg[:,0],1],marker=\"o\",s=10)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.legend([\"Accepted\",\"Rejected\"],loc=0)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network shape: s_l = [2, 3, 3, 1]\n",
            "Training sets used: 90\n",
            "Alpha rate: 1.0 Lambda rate: 0.0\n",
            "Training epochs: 1000\n",
            "\n",
            "Initial weights\n",
            "w^(2) \n",
            "[[0.35541508 0.26051051 0.79114499]\n",
            " [0.99656736 0.92906458 0.83511097]\n",
            " [0.76897662 0.93948707 0.9292253 ]]\n",
            "w^(3) \n",
            "[[0.24438415 0.47181082 0.45869615 0.33296978]\n",
            " [0.41614683 0.26456823 0.15291138 0.59952538]\n",
            " [0.448234   0.94439423 0.55685187 0.25302491]]\n",
            "w^(4) \n",
            "[[0.49328256 0.99358947 0.04218811 0.32065188]]\n",
            "\n",
            "Final weights after training\n",
            "w^(2) \n",
            "[[-1.15755676  1.79410094  2.32473541]\n",
            " [-0.9958215   2.08012724  1.98617363]\n",
            " [-0.92605911  1.93619356  1.92593179]]\n",
            "w^(3) \n",
            "[[-3.43581984  2.48624918  2.27713677  2.22022885]\n",
            " [ 0.85352673 -0.61175618 -0.71468004 -0.27773607]\n",
            " [-0.48345551  1.1926306   0.75425502  0.44618245]]\n",
            "w^(4) \n",
            "[[-2.74910246  5.73359148 -1.57911087  1.10785367]]\n",
            "\n",
            "Costs for all training epochs\n",
            "[0.1738517463964366, 0.16730201556657767, 0.16066732612325504, 0.1541933300369679, 0.1481430645957081, 0.142748088562111, 0.13816228697121757, 0.13443828498416682, 0.13153484729733433, 0.12934691807967177, 0.12774150956508493, 0.12658614165069046, 0.12576515138660269, 0.12518563062969396, 0.12477696470557957, 0.1244874539054991, 0.12428019966456583, 0.12412932202036386, 0.12401688451422604, 0.1239305573429208, 0.12386191233929085, 0.12380521061375112, 0.12375655487379428, 0.12371330330701064, 0.12367366735313899, 0.12363643708023875, 0.12360079434234215, 0.12356618597105162, 0.12353223786046516, 0.12349869683037482, 0.12346539132223253, 0.12343220484543585, 0.1233990580472385, 0.12336589661082328, 0.12333268308998582, 0.12329939140156382, 0.12326600311152972, 0.1232325049311981, 0.12319888702961977, 0.12316514189632317, 0.12313126357505985, 0.1230972471475922, 0.1230630883859612, 0.12302878351824899, 0.12299432907077797, 0.12295972176177382, 0.12292495842967095, 0.12289003598472747, 0.12285495137632171, 0.12281970157079256, 0.12278428353636692, 0.1227486942328472, 0.12271293060449566, 0.12267698957506136, 0.1226408680442443, 0.12260456288511874, 0.12256807094219958, 0.12253138902993466, 0.12249451393147973, 0.12245744239766054, 0.12242017114605616, 0.12238269686016008, 0.12234501618859092, 0.12230712574433222, 0.12226902210398917, 0.122230701807053, 0.1221921613551667, 0.1221533972113895, 0.12211440579945622, 0.12207518350303016, 0.12203572666494789, 0.12199603158645583, 0.1219560945264379, 0.1219159117006321, 0.12187547928083864, 0.12183479339411658, 0.1217938501219707, 0.12175264549952677, 0.12171117551469691, 0.12166943610733279, 0.121627423168368, 0.12158513253894936, 0.12154256000955592, 0.12149970131910673, 0.12145655215405651, 0.12141310814747909, 0.12136936487813926, 0.12132531786955157, 0.12128096258902712, 0.1212362944467073, 0.12119130879458581, 0.12114600092551626, 0.12110036607220771, 0.12105439940620709, 0.1210080960368681, 0.12096145101030661, 0.12091445930834266, 0.12086711584742914, 0.1208194154775662, 0.12077135298120201, 0.12072292307212017, 0.12067412039431176, 0.12062493952083513, 0.12057537495265988, 0.12052542111749717, 0.12047507236861647, 0.12042432298364662, 0.1203731671633635, 0.120321599030463, 0.12026961262831928, 0.12021720191972927, 0.12016436078564172, 0.12011108302387265, 0.12005736234780587, 0.12000319238507996, 0.11994856667625972, 0.11989347867349413, 0.1198379217391603, 0.11978188914449268, 0.1197253740681994, 0.11966836959506325, 0.11961086871453086, 0.11955286431928706, 0.11949434920381674, 0.11943531606295306, 0.11937575749041358, 0.11931566597732336, 0.11925503391072637, 0.11919385357208427, 0.11913211713576467, 0.11906981666751745, 0.11900694412294069, 0.1189434913459366, 0.11887945006715675, 0.1188148119024398, 0.11874956835123851, 0.11868371079504053, 0.11861723049578003, 0.11855011859424398, 0.1184823661084717, 0.11841396393214935, 0.1183449028330001, 0.11827517345117043, 0.1182047662976139, 0.11813367175247379, 0.11806188006346455, 0.11798938134425334, 0.11791616557284484, 0.11784222258996734, 0.11776754209746329, 0.11769211365668614, 0.11761592668690338, 0.11753897046370816, 0.11746123411744143, 0.11738270663162484, 0.11730337684140789, 0.11722323343203009, 0.11714226493730014, 0.11706045973809401, 0.11697780606087503, 0.11689429197623678, 0.11680990539747184, 0.11672463407916855, 0.11663846561583914, 0.11655138744057934, 0.11646338682376638, 0.11637445087179342, 0.11628456652584691, 0.11619372056072892, 0.11610189958372624, 0.11600909003353224, 0.11591527817922279, 0.11582045011928985, 0.11572459178073857, 0.11562768891824902, 0.1155297271134086, 0.11543069177401831, 0.11533056813347727, 0.11522934125025029, 0.11512699600742252, 0.11502351711234689, 0.11491888909638821, 0.11481309631476927, 0.11470612294652525, 0.11459795299456996, 0.11448857028588132, 0.11437795847181012, 0.11426610102851996, 0.1141529812575624, 0.11403858228659494, 0.11392288707024782, 0.11380587839114598, 0.11368753886109312, 0.11356785092242566, 0.11344679684954156, 0.11332435875061335, 0.11320051856949183, 0.11307525808780822, 0.11294855892728213, 0.11282040255224336, 0.11269077027237652, 0.1125596432456943, 0.11242700248175107, 0.11229282884510083, 0.11215710305901262, 0.11201980570944858, 0.11188091724931502, 0.11174041800299456, 0.1115982881711695, 0.11145450783594249, 0.11130905696626736, 0.11116191542369563, 0.11101306296845004, 0.11086247926583245, 0.1107101438929762, 0.11055603634595144, 0.11040013604723224, 0.11024242235353379, 0.11008287456402968, 0.10992147192895606, 0.109758193658612, 0.10959301893276423, 0.10942592691046309, 0.10925689674027911, 0.10908590757096515, 0.1089129385625537, 0.1087379688978933, 0.10856097779463288, 0.10838194451765734, 0.10820084839198144, 0.1080176688161058, 0.10783238527583917, 0.10764497735859131, 0.10745542476813809, 0.10726370733986172, 0.10706980505646743, 0.10687369806417749, 0.10667536668940092, 0.1064747914558793, 0.10627195310230592, 0.10606683260041455, 0.10585941117353365, 0.10564967031560259, 0.10543759181063961, 0.10522315775265767, 0.10500635056601598, 0.10478715302619963, 0.1045655482810128, 0.10434151987217509, 0.10411505175730441, 0.10388612833227136, 0.10365473445390692, 0.10342085546304541, 0.10318447720787977, 0.10294558606760928, 0.10270416897635397, 0.10246021344730975, 0.10221370759711838, 0.1019646401704218, 0.10171300056457003, 0.10145877885445041, 0.10120196581740382, 0.10094255295819167, 0.10068053253397584, 0.10041589757927276, 0.10014864193083961, 0.09987876025245172, 0.09960624805952507, 0.0993311017435401, 0.09905331859621848, 0.09877289683340608, 0.0984898356186105, 0.09820413508614477, 0.09791579636382368, 0.0976248215951607, 0.09733121396101284, 0.09703497770061742, 0.09673611813196767, 0.0964346416714712, 0.09613055585283617, 0.0958238693451298, 0.09551459196995335, 0.09520273471767914, 0.09488830976269402, 0.09457133047759674, 0.09425181144629366, 0.09392976847594295, 0.0936052186076951, 0.09327818012617992, 0.0929486725676934, 0.0926167167270365, 0.09228233466296407, 0.09194554970220001, 0.0916063864419811, 0.09126487075109169, 0.09092102976935632, 0.09057489190555867, 0.0902264868337607, 0.08987584548799593, 0.0895230000553186, 0.08916798396719001, 0.08881083188918981, 0.08845157970904402, 0.08809026452296337, 0.08772692462029259, 0.08736159946647387, 0.08699432968433209, 0.08662515703369486, 0.0862541243893639, 0.08588127571745872, 0.08550665605015889, 0.08513031145887467, 0.08475228902588045, 0.08437263681445017, 0.08399140383753735, 0.08360864002504725, 0.08322439618975193, 0.08283872399190371, 0.08245167590260535, 0.08206330516599951, 0.08167366576034243, 0.08128281235803181, 0.08089080028465913, 0.08049768547716125, 0.08010352444114827, 0.07970837420748599, 0.07931229228821407, 0.07891533663188198, 0.0785175655783862, 0.07811903781339437, 0.07771981232244053, 0.07731994834477808, 0.07691950532707675, 0.07651854287704861, 0.07611712071708986, 0.07571529863802196, 0.07531313645301677, 0.07491069395178798, 0.07450803085513003, 0.07410520676988298, 0.07370228114440229, 0.07329931322460646, 0.07289636201067697, 0.07249348621447865, 0.07209074421776937, 0.07168819403126132, 0.07128589325459615, 0.07088389903729077, 0.07048226804070776, 0.07008105640110168, 0.06968031969378834, 0.06928011289847857, 0.06888049036581866, 0.06848150578517141, 0.06808321215367015, 0.06768566174657438, 0.06728890608895081, 0.06689299592870104, 0.06649798121095236, 0.06610391105382517, 0.06571083372558693, 0.06531879662319831, 0.06492784625225481, 0.06453802820832298, 0.06414938715966836, 0.0637619668313667, 0.06337580999079043, 0.06299095843445722, 0.06260745297622475, 0.062225333436815085, 0.06184463863464764, 0.06146540637795857, 0.061087673458182695, 0.06071147564457061, 0.060336847680013704, 0.059963823278046766, 0.05959243512099744, 0.059222714859249286, 0.05885469311158557, 0.058488399466578696, 0.058123862484989096, 0.057761109703138636, 0.057400167637219786, 0.05704106178850513, 0.05668381664941763, 0.0563284557104248, 0.05597500146771829, 0.05562347543164069, 0.05527389813582163, 0.05492628914698545, 0.054580667075392546, 0.05423704958587759, 0.05389545340944725, 0.053555894355402485, 0.053218387323947514, 0.052882946319253055, 0.05254958446293758, 0.052218314007933746, 0.05188914635270738, 0.05156209205579647, 0.051237160850640014, 0.05091436166066566, 0.05059370261460696, 0.050275191062022974, 0.04995883358899131, 0.04964463603394922, 0.04933260350365743, 0.04902274038926129, 0.04871505038242712, 0.04840953649153002, 0.048106201057872826, 0.04780504577191479, 0.04750607168949089, 0.04720927924800333, 0.04691466828256699, 0.046622238042092916, 0.046331987205293204, 0.04604391389659319, 0.04575801570193615, 0.045474289684467914, 0.045192732400088434, 0.044913339912859576, 0.0446361078102574, 0.04436103121825962, 0.044088104816258786, 0.043817322851792316, 0.04354867915508212, 0.04328216715337584, 0.04301777988508391, 0.04275551001370582, 0.042495349841540624, 0.042237291323176746, 0.041981326078756834, 0.041727445407013806, 0.04147564029807507, 0.041225901446032, 0.04097821926127226, 0.04073258388257311, 0.0404889851889545, 0.04024741281129027, 0.04000785614367718, 0.03977030435456104, 0.03953474639762029, 0.0393011710224066, 0.039069566784743714, 0.038839922056884676, 0.038612225037429, 0.038386463761000654, 0.038162626107688793, 0.03794069981225224, 0.03772067247309046, 0.03750253156098252, 0.03728626442759627, 0.037071858313770606, 0.03685930035757277, 0.03664857760213373, 0.03643967700326423, 0.03623258543685443, 0.036027289706060145, 0.035823776548278875, 0.03562203264191817, 0.03542204461296048, 0.03522379904132705, 0.0350272824670442, 0.03483248139621591, 0.03463938230680539, 0.034447971654229734, 0.03425823587677061, 0.03407016140080468, 0.03388373464585735, 0.0336989420294831, 0.033515769971976105, 0.033334204900914606, 0.03315423325554242, 0.03297584149099122, 0.03279901608234705, 0.03262374352856442, 0.03245001035623146, 0.03227780312318979, 0.03210710842201202, 0.03193791288334076, 0.031770203179092106, 0.03160396602552715, 0.03143918818619442, 0.03127585647474712, 0.031113957757637544, 0.030953478956692582, 0.030794407051572832, 0.03063672908211886, 0.030480432150587145, 0.030325503423779297, 0.030171930135066756, 0.03001969958631458, 0.0298687991497066, 0.02971921626947501, 0.029570938463537144, 0.02942395332504209, 0.029278248523829504, 0.02913381180780405, 0.02899063100422672, 0.028848694020926925, 0.028707988847436895, 0.028568503556050832, 0.02843022630281178, 0.02829314532842791, 0.028157248959120584, 0.02802252560740684, 0.027888963772817722, 0.027756552042555206, 0.027625279092089543, 0.027495133685698968, 0.027366104676953818, 0.02723818100914694, 0.027111351715672506, 0.02698560592035456, 0.02686093283772767, 0.026737321773270924, 0.02661476212359746, 0.026493243376600584, 0.0263727551115588, 0.026253286999200674, 0.0261348288017315, 0.02601737037282306, 0.025900901657568066, 0.025785412692400436, 0.0256708936049832, 0.025557334614064873, 0.025444726029306093, 0.025333058251077435, 0.025222321770229754, 0.02511250716783817, 0.025003605114921182, 0.024895606372135514, 0.024788501789448105, 0.024682282305786465, 0.024576938948667894, 0.024472462833809203, 0.024368845164717302, 0.024266077232262134, 0.02416415041423237, 0.024063056174875128, 0.02396278606442052, 0.02386333171859145, 0.02376468485810003, 0.02366683728813112, 0.023569780897813544, 0.023473507659680162, 0.023378009629117066, 0.023283278943802853, 0.023189307823138494, 0.02309608856766844, 0.023003613558493688, 0.022911875256677258, 0.022820866202642624, 0.022730579015565945, 0.022641006392762123, 0.022552141109065674, 0.022463976016206654, 0.02237650404218217, 0.022289718190623854, 0.02220361154016196, 0.022118177243786154, 0.022033408528203794, 0.021949298693195802, 0.021865841110970584, 0.021783029225516424, 0.021700856551952613, 0.021619316675879765, 0.021538403252729318, 0.021458110007113066, 0.02137843073217251, 0.021299359288928493, 0.021220889605631524, 0.02114301567711282, 0.021065731564136405, 0.02098903139275261, 0.020912909353652937, 0.020837359701526857, 0.020762376754420338, 0.020687954893096556, 0.020614088560399122, 0.020540772260617485, 0.020468000558855233, 0.02039576808040108, 0.020324069510102898, 0.020252899591744762, 0.020182253127427385, 0.0201121249769518, 0.020042510057206506, 0.0199734033415584, 0.019904799859247226, 0.01983669469478395, 0.01976908298735304, 0.01970195993021861, 0.01963532077013485, 0.01956916080676049, 0.01950347539207749, 0.01943825992981401, 0.019373509874871908, 0.01930922073275834, 0.01924538805902217, 0.019182007458694663, 0.019119074585734842, 0.019056585142479363, 0.01899453487909712, 0.01893291959304853, 0.018871735128549387, 0.018810977376039588, 0.018750642271656494, 0.018690725796713117, 0.01863122397718113, 0.018572132883178515, 0.018513448628462225, 0.018455167369925477, 0.018397285307099952, 0.018339798681662862, 0.018282703776948715, 0.018225996917465955, 0.018169674468418436, 0.018113732835231745, 0.018058168463084202, 0.0180029778364427, 0.01794815747860345, 0.017893703951237263, 0.017839613853939746, 0.01778588382378621, 0.017732510534891232, 0.017679490697972988, 0.017626821059922148, 0.017574498403375563, 0.017522519546294548, 0.0174708813415476, 0.017419580676497994, 0.01736861447259565, 0.017317979684973735, 0.017267673302049646, 0.01721769234513049, 0.017168033868023, 0.017118694956647907, 0.017069672728658628, 0.017020964333064357, 0.016972566949857392, 0.016924477789644843, 0.01687669409328454, 0.01682921313152509, 0.01678203220465028, 0.0167351486421275, 0.016688559802260215, 0.016642263071844767, 0.016596255865830915, 0.016550535626986603, 0.016505099825566603, 0.01645994595898509, 0.01641507155149212, 0.016370474153854026, 0.016326151343037564, 0.016282100721897827, 0.016238319918870057, 0.016194806587664954, 0.01615155840696788, 0.016108573080141565, 0.016065848334932395, 0.016023381923180417, 0.01598117162053269, 0.015939215226160312, 0.01589751056247874, 0.015856055474871605, 0.01581484783141795, 0.0157738855226227, 0.015733166461150607, 0.01569268858156329, 0.015652449840059614, 0.015612448214219306, 0.015572681702749612, 0.015533148325235231, 0.015493846121891228, 0.015454773153319078, 0.01541592750026573, 0.015377307263385598, 0.015338910563005594, 0.015300735538893042, 0.015262780350026478, 0.015225043174369356, 0.015187522208646461, 0.015150215668123251, 0.015113121786387849, 0.01507623881513583, 0.015039565023957601, 0.015003098700128607, 0.014966838148401947, 0.014930781690803786, 0.0148949276664312, 0.014859274431252591, 0.014823820357910616, 0.014788563835527579, 0.014753503269513277, 0.014718637081375191, 0.014683963708531185, 0.014649481604124399, 0.014615189236840591, 0.01458108509072773, 0.014547167665017815, 0.014513435473950974, 0.014479887046601799, 0.014446520926707786, 0.014413335672499946, 0.01438032985653563, 0.01434750206553335, 0.014314850900209675, 0.014282374975118257, 0.01425007291849082, 0.014217943372080143, 0.014185984991004997, 0.01415419644359711, 0.014122576411249923, 0.014091123588269343, 0.014059836681726374, 0.014028714411311493, 0.013997755509190881, 0.013966958719864617, 0.013936322800026393, 0.01390584651842511, 0.013875528655728286, 0.01384536800438699, 0.013815363368502588, 0.013785513563695173, 0.013755817416973504, 0.01372627376660671, 0.01369688146199751, 0.013667639363557006, 0.013638546342581204, 0.013609601281128723, 0.013580803071900389, 0.013552150618120196, 0.013523642833417584, 0.013495278641711458, 0.013467056977095413, 0.01343897678372457, 0.013411037015703647, 0.013383236636976537, 0.013355574621217182, 0.013328049951721854, 0.01330066162130271, 0.013273408632182702, 0.013246289995891789, 0.013219304733164381, 0.013192451873838097, 0.013165730456753743, 0.013139139529656542, 0.013112678149098543, 0.013086345380342281, 0.013060140297265556, 0.013034061982267425, 0.013008109526175348, 0.012982282028153488, 0.012956578595611981, 0.012930998344117579, 0.012905540397305154, 0.012880203886790441, 0.01285498795208366, 0.012829891740504395, 0.012804914407097428, 0.012780055114549506, 0.012755313033107249, 0.012730687340495999, 0.01270617722183966, 0.012681781869581478, 0.01265750048340584, 0.012633332270160934, 0.012609276443782477, 0.012585332225218198, 0.012561498842353342, 0.01253777552993705, 0.012514161529509613, 0.012490656089330589, 0.012467258464307815, 0.012443967915927215, 0.0124207837121835, 0.012397705127511776, 0.012374731442719658, 0.012351861944920649, 0.012329095927467915, 0.012306432689889056, 0.012283871537821634, 0.012261411782949407, 0.012239052742939367, 0.012216793741379494, 0.01219463410771729, 0.012172573177198992, 0.012150610290809548, 0.0121287447952132, 0.012106976042694997, 0.01208530339110272, 0.01206372620378963, 0.012042243849557934, 0.012020855702602825, 0.011999561142457203, 0.011978359553937033, 0.011957250327087398, 0.011936232857129164, 0.011915306544406142, 0.011894470794333069, 0.011873725017344084, 0.011853068628841754, 0.011832501049146833, 0.011812021703448474, 0.011791630021755066, 0.011771325438845678, 0.011751107394222008, 0.011730975332060892, 0.011710928701167394, 0.011690966954928439, 0.011671089551266887, 0.011651295952596298, 0.01163158562577604, 0.011611958042067072, 0.011592412677088102, 0.011572949010772299, 0.01155356652732455, 0.011534264715179127, 0.011515043066957813, 0.011495901079428668, 0.011476838253465052, 0.011457854094005255, 0.011438948110012547, 0.01142011981443565, 0.011401368724169707, 0.01138269436001765, 0.011364096246652017, 0.011345573912577227, 0.011327126890092217, 0.011308754715253574, 0.011290456927839015, 0.011272233071311298, 0.01125408269278258, 0.011236005342979099, 0.011218000576206307, 0.011200067950314358, 0.011182207026663985, 0.011164417370092813, 0.011146698548881928, 0.011129050134722955, 0.011111471702685356, 0.011093962831184245, 0.01107652310194843, 0.011059152099988872, 0.01104184941356746, 0.011024614634166176, 0.011007447356456519, 0.010990347178269385, 0.010973313700565136, 0.01095634652740411, 0.010939445265917403, 0.010922609526277938, 0.010905838921671952, 0.010889133068270673, 0.010872491585202386, 0.010855914094524811, 0.010839400221197656, 0.010822949593055645, 0.01080656184078173, 0.010790236597880573, 0.010773973500652413, 0.010757772188167092, 0.0107416323022385, 0.010725553487399139, 0.010709535390875104, 0.010693577662561219, 0.010677679954996537, 0.01066184192334, 0.01064606322534643, 0.010630343521342796, 0.010614682474204634, 0.010599079749332802, 0.010583535014630495, 0.01056804794048038, 0.010552618199722145, 0.010537245467630168, 0.01052192942189144, 0.010506669742583748, 0.010491466112154077, 0.010476318215397214, 0.010461225739434657, 0.010446188373693597, 0.010431205809886287, 0.010416277741989513, 0.010401403866224339, 0.010386583881036013, 0.010371817487074114, 0.010357104387172935, 0.010342444286331996, 0.010327836891696817, 0.010313281912539872, 0.010298779060241755, 0.010284328048272487, 0.010269928592173086, 0.01025558040953729, 0.010241283219993487, 0.010227036745186775, 0.010212840708761295, 0.010198694836342673, 0.010184598855520696, 0.010170552495832092, 0.010156555488743564, 0.010142607567634966, 0.010128708467782617, 0.01011485792634284, 0.010101055682335667, 0.010087301476628603, 0.010073595051920738, 0.01005993615272686, 0.01004632452536178, 0.01003275991792488, 0.010019242080284678, 0.010005770764063709, 0.00999234572262341, 0.009978966711049277, 0.00996563348613609, 0.00995234580637333, 0.009939103431930697, 0.009925906124643846, 0.009912753648000186, 0.00989964576712485, 0.009886582248766849, 0.009873562861285287, 0.009860587374635762, 0.009847655560356881, 0.009834767191556935, 0.009821922042900675, 0.009809119890596204, 0.009796360512382092, 0.009783643687514462, 0.009770969196754367, 0.009758336822355175, 0.009745746348050114, 0.009733197559039952, 0.009720690241980811, 0.009708224184972022, 0.009695799177544201, 0.009683415010647348, 0.009671071476639157, 0.009658768369273337, 0.009646505483688138, 0.009634282616394913, 0.009622099565266846, 0.009609956129527775, 0.009597852109741078, 0.009585787307798733, 0.009573761526910462, 0.009561774571592938, 0.009549826247659142, 0.009537916362207827, 0.009526044723613025, 0.00951421114151372, 0.009502415426803591, 0.009490657391620836, 0.009478936849338131, 0.009467253614552648, 0.009455607503076177, 0.009443998331925385, 0.009432425919312083, 0.009420890084633657, 0.009409390648463566, 0.009397927432541902, 0.00938650025976612, 0.009375108954181718, 0.009363753340973169, 0.009352433246454787, 0.009341148498061792, 0.009329898924341393, 0.00931868435494396, 0.00930750462061434, 0.009296359553183158, 0.00928524898555826, 0.009274172751716233, 0.00926313068669399, 0.009252122626580404, 0.009241148408508133, 0.00923020787064534, 0.00921930085218765, 0.009208427193350108, 0.009197586735359189, 0.009186779320444975, 0.0091760047918333, 0.009165262993738018, 0.009154553771353331, 0.0091438769708462, 0.009133232439348837, 0.009122620024951162, 0.00911203957669352, 0.009101490944559282, 0.009090973979467596, 0.009080488533266222, 0.00907003445872436, 0.009059611609525639, 0.009049219840261065, 0.009038859006422123, 0.009028528964393885, 0.009018229571448183, 0.0090079606857369, 0.008997722166285235]\n",
            "Validation test\n",
            "Training data[0]: Network output: 0.037687397843292635 Expected output: 0.0\n",
            "Training data[1]: Network output: 0.03774624193905006 Expected output: 0.0\n",
            "Training data[2]: Network output: 0.03781155620133861 Expected output: 0.0\n",
            "Training data[3]: Network output: 0.037884074556929825 Expected output: 0.0\n",
            "Training data[4]: Network output: 0.03796461924158624 Expected output: 0.0\n",
            "Training data[95]: Network output: 0.963132709025077 Expected output: 1.0\n",
            "Training data[96]: Network output: 0.9632009766092023 Expected output: 1.0\n",
            "Training data[97]: Network output: 0.9632624111602193 Expected output: 1.0\n",
            "Training data[98]: Network output: 0.963317716828362 Expected output: 1.0\n",
            "Training data[99]: Network output: 0.963367521519794 Expected output: 1.0\n",
            "Model accuracy: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe423215ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}